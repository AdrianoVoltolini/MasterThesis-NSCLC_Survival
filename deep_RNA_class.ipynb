{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch import nn,optim,save\n",
    "import torch\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dati clinici:\n",
    "clinical_raw = pd.read_csv(\"ClinicalData.csv\").set_index(\"Case ID\")\n",
    "\n",
    "# RNA_seq normalizzati:\n",
    "RNA_seq = pd.read_csv(\"RNAseq_norm.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5004, 130)\n"
     ]
    }
   ],
   "source": [
    "#filter low variance RNAs\n",
    "\n",
    "RNA_low_variance_mask = RNA_seq.T.var() > RNA_seq.T.var().quantile(0.05)\n",
    "\n",
    "RNA_filtered = RNA_seq[RNA_low_variance_mask]\n",
    "\n",
    "print(RNA_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mi recupero il train-test-split che ho fatto sul dataset delle CT\n",
    "with open('split_dictionary.pkl', 'rb') as f:\n",
    "    split_dict = pickle.load(f)\n",
    "\n",
    "train_set = [x[:-7] for x in split_dict[\"train\"]]\n",
    "val_set = [x[:-7] for x in split_dict[\"val\"]]\n",
    "test_set = [x[:-7] for x in split_dict[\"test\"]]\n",
    "\n",
    "all_set = train_set+val_set+test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggiungo al dataset dei dati clinici una colonna che indica il numero di giorni che il paziente è sopravvissuto dal giorno della CT\n",
    "\n",
    "def cambia_date(x):\n",
    "    return datetime.strptime(x,\"%m/%d/%Y\")\n",
    "\n",
    "survival_list = list()\n",
    "clinical_temp = clinical_raw.copy()\n",
    "\n",
    "for sample in clinical_raw.iterrows():\n",
    "\n",
    "    date_last = cambia_date(sample[1][\"Date of Last Known Alive\"])\n",
    "    date_CT = cambia_date(sample[1][\"CT Date\"])\n",
    "    days_CT_surgery = timedelta(days=sample[1][\"Days between CT and surgery\"])\n",
    "\n",
    "    if date_last - date_CT > days_CT_surgery: # caso in cui hanno fatto surgery dopo della CT\n",
    "        delta = date_last - date_CT - days_CT_surgery\n",
    "    else: # caso in cui hanno fatto surgery prima della CT\n",
    "        delta = date_last - date_CT + days_CT_surgery\n",
    "\n",
    "    survival_list.append(delta.days)\n",
    "\n",
    "clinical_temp[\"Days Survived\"] = survival_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Not Collected': 49, 'T2a': 47, 'T1a': 40, 'T1b': 31, 'T3': 21, 'T2b': 10, 'T4': 7, 'Tis': 6})\n",
      "Counter({'N0': 129, 'Not Collected': 49, 'N2': 18, 'N1': 15})\n",
      "Counter({'M0': 157, 'Not Collected': 49, 'M1b': 4, 'M1a': 1})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "print(Counter(clinical_temp['Pathological T stage']))\n",
    "print(Counter(clinical_temp['Pathological N stage']))\n",
    "print(Counter(clinical_temp['Pathological M stage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AMC-001</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMC-002</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMC-003</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMC-004</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMC-005</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R01-159</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R01-160</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R01-161</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R01-162</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R01-163</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         no  yes\n",
       "Case ID         \n",
       "AMC-001   0    1\n",
       "AMC-002   0    1\n",
       "AMC-003   0    1\n",
       "AMC-004   0    1\n",
       "AMC-005   0    1\n",
       "...      ..  ...\n",
       "R01-159   1    0\n",
       "R01-160   0    1\n",
       "R01-161   0    1\n",
       "R01-162   0    1\n",
       "R01-163   1    0\n",
       "\n",
       "[211 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mi recupero i dati clinici che mi interessano, fattorizzo\n",
    "\n",
    "clinical_data = clinical_temp[\"Histology \"]\n",
    "\n",
    "clinical_dummy = pd.get_dummies(clinical_data)[\"Adenocarcinoma\"]\n",
    "clinical_dummy2 = pd.get_dummies(clinical_dummy)\n",
    "\n",
    "clinical_adeno = clinical_dummy2.copy()\n",
    "\n",
    "clinical_adeno.columns = [\"no\",\"yes\"]\n",
    "\n",
    "\n",
    "#clinical_adeno = clinical_dummy2[\"Adenocarcinoma\"]\n",
    "\n",
    "#print(pd.Series(x in clinical_adeno.index for x in RNA_seq.columns).all()) # tutti gli RNA hanno corrispondenti dati clinici\n",
    "\n",
    "clinical_adeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Case ID\n",
       "AMC-001                         Adenocarcinoma\n",
       "AMC-002                         Adenocarcinoma\n",
       "AMC-003                         Adenocarcinoma\n",
       "AMC-004                         Adenocarcinoma\n",
       "AMC-005                         Adenocarcinoma\n",
       "                          ...                 \n",
       "R01-159                Squamous cell carcinoma\n",
       "R01-160                         Adenocarcinoma\n",
       "R01-161                         Adenocarcinoma\n",
       "R01-162                         Adenocarcinoma\n",
       "R01-163    NSCLC NOS (not otherwise specified)\n",
       "Name: Histology , Length: 211, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5004, 100) (5004, 14) (5004, 16)\n"
     ]
    }
   ],
   "source": [
    "#divido in train/val/test rispettando lo split fatto sulle CT\n",
    "# samples che non hanno CT sono messi nel validation set (perché altrimenti erano troppo pochi)\n",
    "\n",
    "train_column = [x for x in train_set if x in RNA_seq.columns]\n",
    "val_column = [x for x in val_set if x in RNA_seq.columns]\n",
    "test_column = [x for x in test_set if x in RNA_seq.columns]\n",
    "\n",
    "all_column = train_column + val_column + test_column\n",
    "\n",
    "RNA_noCT = RNA_filtered.loc[:,~RNA_filtered.columns.isin(all_column)]\n",
    "\n",
    "RNA_train = pd.concat([RNA_filtered.loc[:,train_column],RNA_noCT],axis=1).astype(float)\n",
    "RNA_val = RNA_filtered.loc[:,val_column].astype(float)\n",
    "RNA_test = RNA_filtered.loc[:,test_column].astype(float)\n",
    "\n",
    "print(RNA_train.shape, RNA_val.shape, RNA_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNAModel(torch.nn.Module):\n",
    "  def __init__(self, input_dim):\n",
    "    super(RNAModel, self).__init__()\n",
    "    \n",
    "\n",
    "    self.layers = nn.Sequential(nn.Linear(input_dim, input_dim//100, dtype=float), #5268\n",
    "                           nn.ReLU(inplace=True),\n",
    "                           nn.Linear(input_dim//100,2, dtype=float)\n",
    "                           )\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    # forward the input through the layers\n",
    "    x = self.layers(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split,RNA_train, RNA_val, RNA_test, clinical_adeno):\n",
    "        self.split = split\n",
    "        self.RNA_train = RNA_train\n",
    "        self.RNA_val = RNA_val\n",
    "        self.RNA_test = RNA_test\n",
    "        self.y = clinical_adeno\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.split == \"train\":\n",
    "            return self.RNA_train.shape[1]\n",
    "        elif self.split == \"val\":\n",
    "            return self.RNA_val.shape[1]\n",
    "        elif self.split == \"test\":\n",
    "            return self.RNA_test.shape[1]\n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        if self.split == \"train\":\n",
    "            x = torch.from_numpy(self.RNA_train[RNA_train.columns[idx]].apply(lambda x: x + x*np.random.uniform(-0.001,0.001)).astype(float).to_numpy())\n",
    "            y = torch.from_numpy(self.y.loc[RNA_train.columns[idx],:].astype(float).to_numpy())\n",
    "            return x,y\n",
    "        elif self.split == \"val\":\n",
    "            x = torch.from_numpy(self.RNA_val[RNA_val.columns[idx]].to_numpy())\n",
    "            y = torch.from_numpy(self.y.loc[RNA_val.columns[idx],:].astype(float).to_numpy())\n",
    "            return x,y\n",
    "\n",
    "        elif self.split == \"test\":\n",
    "            x = torch.from_numpy(self.RNA_test[RNA_test.columns[idx]].to_numpy())\n",
    "            y = torch.from_numpy(self.y.loc[RNA_test.columns[idx],:].astype(float).to_numpy())\n",
    "            return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = my_dataset(\"train\", RNA_train, RNA_val, RNA_test, clinical_adeno)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2987012987012987 3.347826086956522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classes = Counter(dataset.y.loc[RNA_train.columns,\"no\"])\n",
    "\n",
    "w_adeno = classes[1]/classes[0]\n",
    "w_other = classes[0]/classes[1]\n",
    "\n",
    "print(w_adeno, w_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNAModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=5004, out_features=50, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creo modello, loss function, optimizer\n",
    "\n",
    "my_model = RNAModel(input_dim=RNA_filtered.shape[0])\n",
    "\n",
    "#randomize initial weights and bias\n",
    "for name, layer in my_model.named_children():\n",
    "    for c_name, c_layer in layer.named_children():\n",
    "        if hasattr(c_layer, \"weight\") == True:\n",
    "            nn.init.xavier_uniform_(c_layer.weight)\n",
    "        if hasattr(c_layer, \"bias\") == True:\n",
    "            c_layer.bias.data.fill_(0.)\n",
    "\n",
    "my_loss = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([w_other, w_adeno]))\n",
    "\n",
    "my_optim = optim.Adam(my_model.parameters(), lr=1e-5)\n",
    "\n",
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "starting epoch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.6150178682679104\n",
      "val loss: 1.4922724342227978\n",
      "\n",
      "starting epoch 1\n",
      "train loss: 1.1024987576147471\n",
      "val loss: 1.0734032124824688\n",
      "\n",
      "starting epoch 2\n",
      "train loss: 0.8351754826276071\n",
      "val loss: 1.0177336802669008\n",
      "\n",
      "starting epoch 3\n",
      "train loss: 0.781864419780513\n",
      "val loss: 1.1736600565104838\n",
      "\n",
      "starting epoch 4\n",
      "train loss: 0.7274375426894036\n",
      "val loss: 1.0565256680793749\n",
      "\n",
      "starting epoch 5\n",
      "train loss: 0.7365928132037445\n",
      "val loss: 0.8972926696286272\n",
      "\n",
      "starting epoch 6\n",
      "train loss: 0.7082170925837247\n",
      "val loss: 0.9438386731421968\n",
      "\n",
      "starting epoch 7\n",
      "train loss: 0.7135022941388028\n",
      "val loss: 0.9698202330156198\n",
      "\n",
      "starting epoch 8\n",
      "train loss: 0.707721611244168\n",
      "val loss: 1.0053351050528023\n",
      "\n",
      "starting epoch 9\n",
      "train loss: 0.7117662272121328\n",
      "val loss: 1.091451714386353\n",
      "\n",
      "starting epoch 10\n",
      "train loss: 0.6773523004431039\n",
      "val loss: 1.11558015620161\n",
      "\n",
      "starting epoch 11\n",
      "train loss: 0.6775445144265407\n",
      "val loss: 1.211276006812249\n",
      "\n",
      "starting epoch 12\n",
      "train loss: 0.6760167519539012\n",
      "val loss: 0.8221737250814849\n",
      "\n",
      "starting epoch 13\n",
      "train loss: 0.6615033696015119\n",
      "val loss: 0.9458691673279914\n",
      "\n",
      "starting epoch 14\n",
      "train loss: 0.6645802577375132\n",
      "val loss: 0.8793478804230552\n",
      "\n",
      "starting epoch 15\n",
      "train loss: 0.6459887702762506\n",
      "val loss: 0.804966301495662\n",
      "\n",
      "starting epoch 16\n",
      "train loss: 0.64515973200003\n",
      "val loss: 0.7580323748938259\n",
      "\n",
      "starting epoch 17\n",
      "train loss: 0.6634870949715068\n",
      "val loss: 1.099400839426378\n",
      "\n",
      "starting epoch 18\n",
      "train loss: 0.6277165694818542\n",
      "val loss: 0.9183415041919687\n",
      "\n",
      "starting epoch 19\n",
      "train loss: 0.636275787622783\n",
      "val loss: 0.877071684222858\n",
      "\n",
      "starting epoch 20\n",
      "train loss: 0.6199194593050847\n",
      "val loss: 1.0023707756726647\n",
      "\n",
      "starting epoch 21\n",
      "train loss: 0.609243499084487\n",
      "val loss: 0.763631737905283\n",
      "\n",
      "starting epoch 22\n",
      "train loss: 0.6103655128078803\n",
      "val loss: 0.7781357788493513\n",
      "\n",
      "starting epoch 23\n",
      "train loss: 0.6051284126545926\n",
      "val loss: 0.82752261335279\n",
      "\n",
      "starting epoch 24\n",
      "train loss: 0.5944686103193746\n",
      "val loss: 1.1375295460616472\n",
      "\n",
      "starting epoch 25\n",
      "train loss: 0.5852803125961181\n",
      "val loss: 0.8382436398450489\n",
      "\n",
      "starting epoch 26\n",
      "train loss: 0.5853324553545652\n",
      "val loss: 0.9808927137103791\n",
      "\n",
      "starting epoch 27\n",
      "train loss: 0.5751420725142837\n",
      "val loss: 0.9155878158323181\n",
      "\n",
      "starting epoch 28\n",
      "train loss: 0.5675965419600052\n",
      "val loss: 0.7565474334495393\n",
      "\n",
      "starting epoch 29\n",
      "train loss: 0.5702566061147222\n",
      "val loss: 0.7714567028445327\n",
      "\n",
      "starting epoch 30\n",
      "train loss: 0.5934487789647038\n",
      "val loss: 0.8474653712605386\n",
      "\n",
      "starting epoch 31\n",
      "train loss: 0.5653872612699675\n",
      "val loss: 0.7679541904107896\n",
      "\n",
      "starting epoch 32\n",
      "train loss: 0.5562853535154583\n",
      "val loss: 0.7870995488463286\n",
      "\n",
      "starting epoch 33\n",
      "train loss: 0.5518006824085266\n",
      "val loss: 0.7172739369005431\n",
      "\n",
      "starting epoch 34\n",
      "train loss: 0.5582451981932368\n",
      "val loss: 0.986954142537048\n",
      "\n",
      "starting epoch 35\n",
      "train loss: 0.5409893509931969\n",
      "val loss: 0.6799531326830298\n",
      "\n",
      "starting epoch 36\n",
      "train loss: 0.5386051468001878\n",
      "val loss: 0.9055541451602296\n",
      "\n",
      "starting epoch 37\n",
      "train loss: 0.5389320781737574\n",
      "val loss: 0.7752780692366061\n",
      "\n",
      "starting epoch 38\n",
      "train loss: 0.5358550142368425\n",
      "val loss: 0.7254015212722549\n",
      "\n",
      "starting epoch 39\n",
      "train loss: 0.5304373258973251\n",
      "val loss: 0.7078450443502506\n",
      "\n",
      "starting epoch 40\n",
      "train loss: 0.5305285620126341\n",
      "val loss: 0.7142196626292321\n",
      "\n",
      "starting epoch 41\n",
      "train loss: 0.5335841222613418\n",
      "val loss: 0.6941894993333917\n",
      "\n",
      "starting epoch 42\n",
      "train loss: 0.5215403363686295\n",
      "val loss: 0.6841489596492653\n",
      "\n",
      "starting epoch 43\n",
      "train loss: 0.5144390462245533\n",
      "val loss: 0.665999987801251\n",
      "\n",
      "starting epoch 44\n",
      "train loss: 0.510383627873277\n",
      "val loss: 0.8039976726085827\n",
      "\n",
      "starting epoch 45\n",
      "train loss: 0.5105275391418386\n",
      "val loss: 0.700932821581688\n",
      "\n",
      "starting epoch 46\n",
      "train loss: 0.511391031490808\n",
      "val loss: 0.6917084774286681\n",
      "\n",
      "starting epoch 47\n",
      "train loss: 0.5016206612775225\n",
      "val loss: 0.7077285848451282\n",
      "\n",
      "starting epoch 48\n",
      "train loss: 0.5051569820376707\n",
      "val loss: 0.7982547605539618\n",
      "\n",
      "starting epoch 49\n",
      "train loss: 0.4979737448365288\n",
      "val loss: 0.7556228067770007\n",
      "\n",
      "starting epoch 50\n",
      "train loss: 0.4962367567200682\n",
      "val loss: 0.792339409395731\n",
      "\n",
      "starting epoch 51\n",
      "train loss: 0.5057893903332749\n",
      "val loss: 0.8823142588943688\n",
      "\n",
      "starting epoch 52\n",
      "train loss: 0.5203376459748319\n",
      "val loss: 0.6742991235646913\n",
      "\n",
      "starting epoch 53\n",
      "train loss: 0.4990867265095228\n",
      "val loss: 0.6528634713953305\n",
      "\n",
      "starting epoch 54\n",
      "train loss: 0.49216838995941725\n",
      "val loss: 0.786160283324659\n",
      "\n",
      "starting epoch 55\n",
      "train loss: 0.4812820231794056\n",
      "val loss: 0.7458515117677258\n",
      "\n",
      "starting epoch 56\n",
      "train loss: 0.48224005866222397\n",
      "val loss: 0.6457185596780256\n",
      "\n",
      "starting epoch 57\n",
      "train loss: 0.4957887516527684\n",
      "val loss: 0.7706378038229902\n",
      "\n",
      "starting epoch 58\n",
      "train loss: 0.47691788050549827\n",
      "val loss: 0.6551775236737506\n",
      "\n",
      "starting epoch 59\n",
      "train loss: 0.4764499267314449\n",
      "val loss: 0.7604539488225344\n",
      "\n",
      "starting epoch 60\n",
      "train loss: 0.47946152721521623\n",
      "val loss: 0.5975232924361469\n",
      "\n",
      "starting epoch 61\n",
      "train loss: 0.4709925610621763\n",
      "val loss: 0.6180955808221192\n",
      "\n",
      "starting epoch 62\n",
      "train loss: 0.48185181642212693\n",
      "val loss: 0.6949278516692154\n",
      "\n",
      "starting epoch 63\n",
      "train loss: 0.45911918485042963\n",
      "val loss: 0.6374381596752337\n",
      "\n",
      "starting epoch 64\n",
      "train loss: 0.4611371483378299\n",
      "val loss: 0.8921304588972869\n",
      "\n",
      "starting epoch 65\n",
      "train loss: 0.45877208503808653\n",
      "val loss: 0.7669446293630585\n",
      "\n",
      "starting epoch 66\n",
      "train loss: 0.4604277305732161\n",
      "val loss: 0.9197111850419655\n",
      "\n",
      "starting epoch 67\n",
      "train loss: 0.45646840038074077\n",
      "val loss: 0.7131200157291885\n",
      "\n",
      "starting epoch 68\n",
      "train loss: 0.4474720563096917\n",
      "val loss: 0.7602458399702532\n",
      "\n",
      "starting epoch 69\n",
      "train loss: 0.4482840329387404\n",
      "val loss: 0.5813929384535005\n",
      "\n",
      "starting epoch 70\n",
      "train loss: 0.4454912160133809\n",
      "val loss: 0.7369289438685627\n",
      "\n",
      "starting epoch 71\n",
      "train loss: 0.45330064766607164\n",
      "val loss: 0.6258188315905433\n",
      "\n",
      "starting epoch 72\n",
      "train loss: 0.4391968388701531\n",
      "val loss: 0.5535254759988336\n",
      "\n",
      "starting epoch 73\n",
      "train loss: 0.43560697669630316\n",
      "val loss: 0.6116381957125899\n",
      "\n",
      "starting epoch 74\n",
      "train loss: 0.43316116816415773\n",
      "val loss: 0.5843129008643706\n",
      "\n",
      "starting epoch 75\n",
      "train loss: 0.4428735292076258\n",
      "val loss: 0.8168214166485617\n",
      "\n",
      "starting epoch 76\n",
      "train loss: 0.4346723914369749\n",
      "val loss: 0.7562019989552758\n",
      "\n",
      "starting epoch 77\n",
      "train loss: 0.42495039319679756\n",
      "val loss: 0.6162772883052321\n",
      "\n",
      "starting epoch 78\n",
      "train loss: 0.4300912013656341\n",
      "val loss: 0.7305059024481433\n",
      "\n",
      "starting epoch 79\n",
      "train loss: 0.4326179126001004\n",
      "val loss: 0.6770284648497611\n",
      "\n",
      "starting epoch 80\n",
      "train loss: 0.41915747502939527\n",
      "val loss: 0.8186535355958193\n",
      "\n",
      "starting epoch 81\n",
      "train loss: 0.4335104563317104\n",
      "val loss: 0.7125049348747234\n",
      "\n",
      "starting epoch 82\n",
      "train loss: 0.4185291534491077\n",
      "val loss: 0.6113708709195317\n",
      "\n",
      "starting epoch 83\n",
      "train loss: 0.41631010744605346\n",
      "val loss: 0.6484270661277303\n",
      "\n",
      "starting epoch 84\n",
      "train loss: 0.41505551445094013\n",
      "val loss: 0.8633297031662976\n",
      "\n",
      "starting epoch 85\n",
      "train loss: 0.41090993109920476\n",
      "val loss: 0.5660858999388901\n",
      "\n",
      "starting epoch 86\n",
      "train loss: 0.40664267527501285\n",
      "val loss: 0.7080340159146394\n",
      "\n",
      "starting epoch 87\n",
      "train loss: 0.4089500308385697\n",
      "val loss: 0.5801104065868125\n",
      "\n",
      "starting epoch 88\n",
      "train loss: 0.42307410519728395\n",
      "val loss: 0.5813569772621128\n",
      "\n",
      "starting epoch 89\n",
      "train loss: 0.42591868267726457\n",
      "val loss: 0.7395191575555149\n",
      "\n",
      "starting epoch 90\n",
      "train loss: 0.4230790528131223\n",
      "val loss: 0.782043717267635\n",
      "\n",
      "starting epoch 91\n",
      "train loss: 0.40166186860572123\n",
      "val loss: 0.6081158657955141\n",
      "\n",
      "starting epoch 92\n",
      "train loss: 0.396822999587586\n",
      "val loss: 0.7202617207018078\n",
      "\n",
      "starting epoch 93\n",
      "train loss: 0.3961055337574811\n",
      "val loss: 0.7345691371913787\n",
      "\n",
      "starting epoch 94\n",
      "train loss: 0.4004529078502167\n",
      "val loss: 0.795976534653162\n",
      "\n",
      "starting epoch 95\n",
      "train loss: 0.3886696240548068\n",
      "val loss: 0.7870032811065051\n",
      "\n",
      "starting epoch 96\n",
      "train loss: 0.40766788504396906\n",
      "val loss: 0.5838639249033573\n",
      "\n",
      "starting epoch 97\n",
      "train loss: 0.3902017704657893\n",
      "val loss: 0.6995206133198942\n",
      "\n",
      "starting epoch 98\n",
      "train loss: 0.38358346261374204\n",
      "val loss: 0.614570728067755\n",
      "\n",
      "starting epoch 99\n",
      "train loss: 0.3842084405048386\n",
      "val loss: 0.5196171580899563\n",
      "\n",
      "starting epoch 100\n",
      "train loss: 0.37753431415570315\n",
      "val loss: 0.6863002562022832\n",
      "\n",
      "starting epoch 101\n",
      "train loss: 0.38036187749539463\n",
      "val loss: 0.7105030504859151\n",
      "\n",
      "starting epoch 102\n",
      "train loss: 0.37756178477646424\n",
      "val loss: 0.7735561700224092\n",
      "\n",
      "starting epoch 103\n",
      "train loss: 0.37815007709317405\n",
      "val loss: 0.7134452966860156\n",
      "\n",
      "starting epoch 104\n",
      "train loss: 0.3753873725087374\n",
      "val loss: 0.5833289760576863\n",
      "\n",
      "starting epoch 105\n",
      "train loss: 0.37524792193853285\n",
      "val loss: 0.6449182410443743\n",
      "\n",
      "starting epoch 106\n",
      "train loss: 0.37341063745600084\n",
      "val loss: 0.5233938811369545\n",
      "\n",
      "starting epoch 107\n",
      "train loss: 0.37314317143230646\n",
      "val loss: 0.6727794824472607\n",
      "\n",
      "starting epoch 108\n",
      "train loss: 0.3681681759380268\n",
      "val loss: 0.569333077039656\n",
      "\n",
      "starting epoch 109\n",
      "train loss: 0.365523503988593\n",
      "val loss: 0.6616288948627336\n",
      "\n",
      "starting epoch 110\n",
      "train loss: 0.3630588827400308\n",
      "val loss: 0.5437331763156155\n",
      "\n",
      "starting epoch 111\n",
      "train loss: 0.36302290793068037\n",
      "val loss: 0.5045859663211857\n",
      "\n",
      "starting epoch 112\n",
      "train loss: 0.35898265295365017\n",
      "val loss: 0.633768780874411\n",
      "\n",
      "starting epoch 113\n",
      "train loss: 0.3623464301033027\n",
      "val loss: 0.6566817607171733\n",
      "\n",
      "starting epoch 114\n",
      "train loss: 0.35555309374469024\n",
      "val loss: 0.5772120945073613\n",
      "\n",
      "starting epoch 115\n",
      "train loss: 0.3545561476876219\n",
      "val loss: 0.6978310564110479\n",
      "\n",
      "starting epoch 116\n",
      "train loss: 0.35509520765122776\n",
      "val loss: 0.6678583576834556\n",
      "\n",
      "starting epoch 117\n",
      "train loss: 0.35690424559340894\n",
      "val loss: 0.5022877835746216\n",
      "\n",
      "starting epoch 118\n",
      "train loss: 0.3514199560147585\n",
      "val loss: 0.5771005054655561\n",
      "\n",
      "starting epoch 119\n",
      "train loss: 0.34752852539317663\n",
      "val loss: 0.6617627809375399\n",
      "\n",
      "starting epoch 120\n",
      "train loss: 0.34407499537823194\n",
      "val loss: 0.5471151965490633\n",
      "\n",
      "starting epoch 121\n",
      "train loss: 0.34562442774918045\n",
      "val loss: 0.6884975457647244\n",
      "\n",
      "starting epoch 122\n",
      "train loss: 0.35098680561399764\n",
      "val loss: 0.640647544841618\n",
      "\n",
      "starting epoch 123\n",
      "train loss: 0.3423531471112162\n",
      "val loss: 0.5269513976537402\n",
      "\n",
      "starting epoch 124\n",
      "train loss: 0.3405592722618827\n",
      "val loss: 0.4663167977026028\n",
      "\n",
      "starting epoch 125\n",
      "train loss: 0.34809196720453606\n",
      "val loss: 0.4959808369320023\n",
      "\n",
      "starting epoch 126\n",
      "train loss: 0.3363429530439225\n",
      "val loss: 0.6501056039316752\n",
      "\n",
      "starting epoch 127\n",
      "train loss: 0.346372797485478\n",
      "val loss: 0.49415024361495663\n",
      "\n",
      "starting epoch 128\n",
      "train loss: 0.36169021857637507\n",
      "val loss: 0.6285245954100231\n",
      "\n",
      "starting epoch 129\n",
      "train loss: 0.32853406130207863\n",
      "val loss: 0.7476853977611291\n",
      "\n",
      "starting epoch 130\n",
      "train loss: 0.38005284718567517\n",
      "val loss: 0.7243575712495912\n",
      "\n",
      "starting epoch 131\n",
      "train loss: 0.3636430965277602\n",
      "val loss: 0.5408852190190624\n",
      "\n",
      "starting epoch 132\n",
      "train loss: 0.3379031382067575\n",
      "val loss: 0.5637500332517651\n",
      "\n",
      "starting epoch 133\n",
      "train loss: 0.335504637232459\n",
      "val loss: 0.5283404532221448\n",
      "\n",
      "starting epoch 134\n",
      "train loss: 0.32889668246851744\n",
      "val loss: 0.7783605082660265\n",
      "\n",
      "starting epoch 135\n",
      "train loss: 0.3261139538643545\n",
      "val loss: 0.5928241863515846\n",
      "\n",
      "starting epoch 136\n",
      "train loss: 0.32293925725165046\n",
      "val loss: 0.7293875525745909\n",
      "\n",
      "starting epoch 137\n",
      "train loss: 0.3274212748380136\n",
      "val loss: 0.5373984273129475\n",
      "\n",
      "starting epoch 138\n",
      "train loss: 0.3250060778375609\n",
      "val loss: 0.7203586267171692\n",
      "\n",
      "starting epoch 139\n",
      "train loss: 0.32130455020021387\n",
      "val loss: 0.8786623303383878\n",
      "\n",
      "starting epoch 140\n",
      "train loss: 0.32766592408979195\n",
      "val loss: 0.6874966333537795\n",
      "\n",
      "starting epoch 141\n",
      "train loss: 0.3169582761081212\n",
      "val loss: 0.5778583071565409\n",
      "\n",
      "starting epoch 142\n",
      "train loss: 0.32098106609851584\n",
      "val loss: 0.699222257872709\n",
      "\n",
      "starting epoch 143\n",
      "train loss: 0.32036364981435594\n",
      "val loss: 0.4784273614849298\n",
      "\n",
      "starting epoch 144\n",
      "train loss: 0.3185487314972485\n",
      "val loss: 0.6930576334313727\n",
      "\n",
      "starting epoch 145\n",
      "train loss: 0.313326311684097\n",
      "val loss: 0.48471682469968713\n",
      "\n",
      "starting epoch 146\n",
      "train loss: 0.3138113823150967\n",
      "val loss: 0.47758562666484616\n",
      "\n",
      "starting epoch 147\n",
      "train loss: 0.31209967676327033\n",
      "val loss: 0.6228061943163803\n",
      "\n",
      "starting epoch 148\n",
      "train loss: 0.3083570764976674\n",
      "val loss: 0.6397834556986117\n",
      "\n",
      "starting epoch 149\n",
      "train loss: 0.30930373408636347\n",
      "val loss: 0.5680570837911028\n",
      "\n",
      "starting epoch 150\n",
      "train loss: 0.310920282804901\n",
      "val loss: 0.7546292927091809\n",
      "\n",
      "starting epoch 151\n",
      "train loss: 0.31639297932688465\n",
      "val loss: 0.5128551816799705\n",
      "\n",
      "starting epoch 152\n",
      "train loss: 0.3195821652147031\n",
      "val loss: 0.550902696624874\n",
      "\n",
      "starting epoch 153\n",
      "train loss: 0.3006817158672102\n",
      "val loss: 0.4348074845439099\n",
      "\n",
      "starting epoch 154\n",
      "train loss: 0.30782160360947064\n",
      "val loss: 0.5377386798009403\n",
      "\n",
      "starting epoch 155\n",
      "train loss: 0.3048408894836783\n",
      "val loss: 0.5096422528204939\n",
      "\n",
      "starting epoch 156\n",
      "train loss: 0.29921485902976713\n",
      "val loss: 0.7271192763628681\n",
      "\n",
      "starting epoch 157\n",
      "train loss: 0.3059913621229993\n",
      "val loss: 0.43476057241741506\n",
      "\n",
      "starting epoch 158\n",
      "train loss: 0.30320758902679923\n",
      "val loss: 0.7445019760463136\n",
      "\n",
      "starting epoch 159\n",
      "train loss: 0.29942082963616345\n",
      "val loss: 0.6434641566550339\n",
      "\n",
      "starting epoch 160\n",
      "train loss: 0.29200542857173284\n",
      "val loss: 0.5546870752647846\n",
      "\n",
      "starting epoch 161\n",
      "train loss: 0.29625107160395314\n",
      "val loss: 0.46485877325737224\n",
      "\n",
      "starting epoch 162\n",
      "train loss: 0.30415344675470035\n",
      "val loss: 0.6118750118857887\n",
      "\n",
      "starting epoch 163\n",
      "train loss: 0.31529607300234236\n",
      "val loss: 0.6099670380526099\n",
      "\n",
      "starting epoch 164\n",
      "train loss: 0.2927016657314851\n",
      "val loss: 0.49685482827473865\n",
      "\n",
      "starting epoch 165\n",
      "train loss: 0.2964203675874411\n",
      "val loss: 0.629972924916486\n",
      "\n",
      "starting epoch 166\n",
      "train loss: 0.28680507921013637\n",
      "val loss: 0.5217718632784147\n",
      "\n",
      "starting epoch 167\n",
      "train loss: 0.2873348015185539\n",
      "val loss: 0.6937947677444056\n",
      "\n",
      "starting epoch 168\n",
      "train loss: 0.2897080744430881\n",
      "val loss: 0.4975154975012839\n",
      "\n",
      "starting epoch 169\n",
      "train loss: 0.28505695678667814\n",
      "val loss: 0.6576056815396056\n",
      "\n",
      "starting epoch 170\n",
      "train loss: 0.28435498547183324\n",
      "val loss: 0.4678192887815632\n",
      "\n",
      "starting epoch 171\n",
      "train loss: 0.28331880929893594\n",
      "val loss: 0.7172752587454687\n",
      "\n",
      "starting epoch 172\n",
      "train loss: 0.2822082843496029\n",
      "val loss: 0.46813270611603486\n",
      "\n",
      "starting epoch 173\n",
      "train loss: 0.28147054989401266\n",
      "val loss: 0.6625590025457977\n",
      "\n",
      "starting epoch 174\n",
      "train loss: 0.2883418556431089\n",
      "val loss: 0.584073246586251\n",
      "\n",
      "starting epoch 175\n",
      "train loss: 0.2799171013783267\n",
      "val loss: 0.491880033495338\n",
      "\n",
      "starting epoch 176\n",
      "train loss: 0.2863275781586582\n",
      "val loss: 0.4515337929230071\n",
      "\n",
      "starting epoch 177\n",
      "train loss: 0.2828373913864711\n",
      "val loss: 0.5079330202803853\n",
      "\n",
      "starting epoch 178\n",
      "train loss: 0.2746424617282748\n",
      "val loss: 0.4770764659219089\n",
      "\n",
      "starting epoch 179\n",
      "train loss: 0.27473005939390943\n",
      "val loss: 0.633337970213036\n",
      "\n",
      "starting epoch 180\n",
      "train loss: 0.2745412465920733\n",
      "val loss: 0.5096726056704672\n",
      "\n",
      "starting epoch 181\n",
      "train loss: 0.28947127238168535\n",
      "val loss: 0.5265219450063605\n",
      "\n",
      "starting epoch 182\n",
      "train loss: 0.2820927144595855\n",
      "val loss: 0.6013003385962747\n",
      "\n",
      "starting epoch 183\n",
      "train loss: 0.2817920466264815\n",
      "val loss: 0.4637629038055211\n",
      "\n",
      "starting epoch 184\n",
      "train loss: 0.2761311640127024\n",
      "val loss: 0.533309031945806\n",
      "\n",
      "starting epoch 185\n",
      "train loss: 0.2710440146201996\n",
      "val loss: 0.6478242614941255\n",
      "\n",
      "starting epoch 186\n",
      "train loss: 0.2756740130449157\n",
      "val loss: 0.47579662144445395\n",
      "\n",
      "starting epoch 187\n",
      "train loss: 0.26296990831141637\n",
      "val loss: 0.7345428581493838\n",
      "\n",
      "starting epoch 188\n",
      "train loss: 0.271256293638986\n",
      "val loss: 0.45074652115312974\n",
      "\n",
      "starting epoch 189\n",
      "train loss: 0.26829122082302215\n",
      "val loss: 0.6492553257767018\n",
      "\n",
      "starting epoch 190\n",
      "train loss: 0.2631430491932512\n",
      "val loss: 0.516143327764029\n",
      "\n",
      "starting epoch 191\n",
      "train loss: 0.2650273956447694\n",
      "val loss: 0.5930911391516805\n",
      "\n",
      "starting epoch 192\n",
      "train loss: 0.26323610949618026\n",
      "val loss: 0.7278001474382719\n",
      "\n",
      "starting epoch 193\n",
      "train loss: 0.262355626442646\n",
      "val loss: 0.44659344394805384\n",
      "\n",
      "starting epoch 194\n",
      "train loss: 0.26110397372915445\n",
      "val loss: 0.5082880744414331\n",
      "\n",
      "starting epoch 195\n",
      "train loss: 0.25955466766839164\n",
      "val loss: 0.5804518401371195\n",
      "\n",
      "starting epoch 196\n",
      "train loss: 0.27369451517584886\n",
      "val loss: 0.4468721668419504\n",
      "\n",
      "starting epoch 197\n",
      "train loss: 0.2602623832369002\n",
      "val loss: 0.6192341062434783\n",
      "\n",
      "starting epoch 198\n",
      "train loss: 0.2628227240274329\n",
      "val loss: 0.4820981695997241\n",
      "\n",
      "starting epoch 199\n",
      "train loss: 0.25601364517942216\n",
      "val loss: 0.4629359884160523\n",
      "\n",
      "starting epoch 200\n",
      "train loss: 0.25395099890936557\n",
      "val loss: 0.4397672023300184\n",
      "\n",
      "starting epoch 201\n",
      "train loss: 0.2570615470595698\n",
      "val loss: 0.7217641552701165\n",
      "\n",
      "starting epoch 202\n",
      "train loss: 0.2561162054533147\n",
      "val loss: 0.4455260790030121\n",
      "\n",
      "starting epoch 203\n",
      "train loss: 0.25596955960124024\n",
      "val loss: 0.64994004772597\n",
      "\n",
      "starting epoch 204\n",
      "train loss: 0.26097604941778074\n",
      "val loss: 0.4638798864895901\n",
      "\n",
      "starting epoch 205\n",
      "train loss: 0.2518028035509576\n",
      "val loss: 0.6759498777520225\n",
      "\n",
      "starting epoch 206\n",
      "train loss: 0.2511069748738791\n",
      "val loss: 0.5897364517457697\n",
      "\n",
      "starting epoch 207\n",
      "train loss: 0.24585055770913516\n",
      "val loss: 0.6907435837638929\n",
      "\n",
      "starting epoch 208\n",
      "train loss: 0.25638903077494696\n",
      "val loss: 0.42502997710857393\n",
      "\n",
      "starting epoch 209\n",
      "train loss: 0.2620451774677878\n",
      "val loss: 0.46092001636514596\n",
      "\n",
      "starting epoch 210\n",
      "train loss: 0.24544485607930694\n",
      "val loss: 0.6482569469934027\n",
      "\n",
      "starting epoch 211\n",
      "train loss: 0.25022500548005455\n",
      "val loss: 0.5114110632139033\n",
      "\n",
      "starting epoch 212\n",
      "train loss: 0.24442000692925675\n",
      "val loss: 0.5798297123555911\n",
      "\n",
      "starting epoch 213\n",
      "train loss: 0.24400575529650417\n",
      "val loss: 0.46983466146358366\n",
      "\n",
      "starting epoch 214\n",
      "train loss: 0.2443233943482757\n",
      "val loss: 0.4109085368162243\n",
      "\n",
      "starting epoch 215\n",
      "train loss: 0.24266510762447044\n",
      "val loss: 0.4991587361971408\n",
      "\n",
      "starting epoch 216\n",
      "train loss: 0.24432983016427925\n",
      "val loss: 0.45873176057786086\n",
      "\n",
      "starting epoch 217\n",
      "train loss: 0.2413281023978595\n",
      "val loss: 0.5113468485903215\n",
      "\n",
      "starting epoch 218\n",
      "train loss: 0.24205141113871692\n",
      "val loss: 0.6609624988538456\n",
      "\n",
      "starting epoch 219\n",
      "train loss: 0.23723264390977494\n",
      "val loss: 0.5198733996592371\n",
      "\n",
      "starting epoch 220\n",
      "train loss: 0.24033052910732505\n",
      "val loss: 0.6734050948487347\n",
      "\n",
      "starting epoch 221\n",
      "train loss: 0.2363949911182333\n",
      "val loss: 0.4471375324603119\n",
      "\n",
      "starting epoch 222\n",
      "train loss: 0.2428487978918427\n",
      "val loss: 0.6835496095335925\n",
      "\n",
      "starting epoch 223\n",
      "train loss: 0.23605612741021026\n",
      "val loss: 0.4000401383618706\n",
      "\n",
      "starting epoch 224\n",
      "train loss: 0.23419563875895646\n",
      "val loss: 0.4671566398376624\n",
      "\n",
      "starting epoch 225\n",
      "train loss: 0.23448904380956623\n",
      "val loss: 0.6817103791325485\n",
      "\n",
      "starting epoch 226\n",
      "train loss: 0.23842058670229083\n",
      "val loss: 0.6922486890795814\n",
      "\n",
      "starting epoch 227\n",
      "train loss: 0.23267396165089188\n",
      "val loss: 0.6363011971056763\n",
      "\n",
      "starting epoch 228\n",
      "train loss: 0.23140478666431924\n",
      "val loss: 0.47529111124053064\n",
      "\n",
      "starting epoch 229\n",
      "train loss: 0.2348023228532632\n",
      "val loss: 0.693091315062852\n",
      "\n",
      "starting epoch 230\n",
      "train loss: 0.2268082996532988\n",
      "val loss: 0.41696693819954495\n",
      "\n",
      "starting epoch 231\n",
      "train loss: 0.23217471202851742\n",
      "val loss: 0.515121057163284\n",
      "\n",
      "starting epoch 232\n",
      "train loss: 0.2312664523819395\n",
      "val loss: 0.420404060677629\n",
      "\n",
      "starting epoch 233\n",
      "train loss: 0.23003724185829172\n",
      "val loss: 0.443395773563338\n",
      "\n",
      "starting epoch 234\n",
      "train loss: 0.2321542514444686\n",
      "val loss: 0.3864451415116762\n",
      "\n",
      "starting epoch 235\n",
      "train loss: 0.2244437196563481\n",
      "val loss: 0.7128252469822135\n",
      "\n",
      "starting epoch 236\n",
      "train loss: 0.229912384482078\n",
      "val loss: 0.6010118229723854\n",
      "\n",
      "starting epoch 237\n",
      "train loss: 0.22401972422639277\n",
      "val loss: 0.39835226818973024\n",
      "\n",
      "starting epoch 238\n",
      "train loss: 0.2223555441961116\n",
      "val loss: 0.6471247997423177\n",
      "\n",
      "starting epoch 239\n",
      "train loss: 0.22545919314421242\n",
      "val loss: 0.4681761803206197\n",
      "\n",
      "starting epoch 240\n",
      "train loss: 0.22263552589067212\n",
      "val loss: 0.626004266134515\n",
      "\n",
      "starting epoch 241\n",
      "train loss: 0.23024560319540016\n",
      "val loss: 0.4721218661957734\n",
      "\n",
      "starting epoch 242\n",
      "train loss: 0.2207423761234173\n",
      "val loss: 0.4530638498399211\n",
      "\n",
      "starting epoch 243\n",
      "train loss: 0.21983453861031083\n",
      "val loss: 0.40167833007969644\n",
      "\n",
      "starting epoch 244\n",
      "train loss: 0.2229313629938837\n",
      "val loss: 0.4497266995367951\n",
      "\n",
      "starting epoch 245\n",
      "train loss: 0.2172215793396688\n",
      "val loss: 0.40472972767746374\n",
      "\n",
      "starting epoch 246\n",
      "train loss: 0.2190284219304083\n",
      "val loss: 0.4899253326961531\n",
      "\n",
      "starting epoch 247\n",
      "train loss: 0.2200551536915086\n",
      "val loss: 0.7408254711182672\n",
      "\n",
      "starting epoch 248\n",
      "train loss: 0.22444514149542752\n",
      "val loss: 0.6776456548767192\n",
      "\n",
      "starting epoch 249\n",
      "train loss: 0.21970871353210591\n",
      "val loss: 0.7254774500913694\n",
      "\n",
      "starting epoch 250\n",
      "train loss: 0.21276466896583082\n",
      "val loss: 0.6415042824536024\n",
      "\n",
      "starting epoch 251\n",
      "train loss: 0.22073288615552816\n",
      "val loss: 0.6471680270406135\n",
      "\n",
      "starting epoch 252\n",
      "train loss: 0.2148692641812131\n",
      "val loss: 0.4207708871865658\n",
      "\n",
      "starting epoch 253\n",
      "train loss: 0.2121436728133542\n",
      "val loss: 0.4256626062829707\n",
      "\n",
      "starting epoch 254\n",
      "train loss: 0.21288799311649442\n",
      "val loss: 0.667095211257344\n",
      "\n",
      "starting epoch 255\n",
      "train loss: 0.22110971383991176\n",
      "val loss: 0.5222343572035466\n",
      "\n",
      "starting epoch 256\n",
      "train loss: 0.20998030735973652\n",
      "val loss: 0.4125203925318599\n",
      "\n",
      "starting epoch 257\n",
      "train loss: 0.2095160790981382\n",
      "val loss: 0.45266006946465215\n",
      "\n",
      "starting epoch 258\n",
      "train loss: 0.21178827942087292\n",
      "val loss: 0.4090036453550561\n",
      "\n",
      "starting epoch 259\n",
      "train loss: 0.22726547802272487\n",
      "val loss: 0.46090331861641914\n",
      "\n",
      "starting epoch 260\n",
      "train loss: 0.2172685791958139\n",
      "val loss: 0.49812279849896424\n",
      "\n",
      "starting epoch 261\n",
      "train loss: 0.2067103520458684\n",
      "val loss: 0.39366967002312847\n",
      "\n",
      "starting epoch 262\n",
      "train loss: 0.20730158903898194\n",
      "val loss: 0.46975535099745325\n",
      "\n",
      "starting epoch 263\n",
      "train loss: 0.2061153697377674\n",
      "val loss: 0.6698364188739432\n",
      "\n",
      "starting epoch 264\n",
      "train loss: 0.2054286578571439\n",
      "val loss: 0.47295635982103745\n",
      "\n",
      "starting epoch 265\n",
      "train loss: 0.20883345456856145\n",
      "val loss: 0.7508305034834643\n",
      "\n",
      "starting epoch 266\n",
      "train loss: 0.2048453506701402\n",
      "val loss: 0.5642608167733332\n",
      "\n",
      "starting epoch 267\n",
      "train loss: 0.20365640275104407\n",
      "val loss: 0.49265132312579824\n",
      "\n",
      "starting epoch 268\n",
      "train loss: 0.21029518224041435\n",
      "val loss: 0.42043055953663366\n",
      "\n",
      "starting epoch 269\n",
      "train loss: 0.2110316661052763\n",
      "val loss: 0.6700833716709109\n",
      "\n",
      "starting epoch 270\n",
      "train loss: 0.20030255239786593\n",
      "val loss: 0.48873672604274576\n",
      "\n",
      "starting epoch 271\n",
      "train loss: 0.20777434986864413\n",
      "val loss: 0.6355921947260321\n",
      "\n",
      "starting epoch 272\n",
      "train loss: 0.19900991050176012\n",
      "val loss: 0.46438735491263905\n",
      "\n",
      "starting epoch 273\n",
      "train loss: 0.20213061484783346\n",
      "val loss: 0.38126107137720266\n",
      "\n",
      "starting epoch 274\n",
      "train loss: 0.20688360080266394\n",
      "val loss: 0.503326002069207\n",
      "\n",
      "starting epoch 275\n",
      "train loss: 0.20538558755764455\n",
      "val loss: 0.4433307785018272\n",
      "\n",
      "starting epoch 276\n",
      "train loss: 0.19986585428510922\n",
      "val loss: 0.46270940811311245\n",
      "\n",
      "starting epoch 277\n",
      "train loss: 0.19855296738624756\n",
      "val loss: 0.4844697187629128\n",
      "\n",
      "starting epoch 278\n",
      "train loss: 0.1957750747836931\n",
      "val loss: 0.42136290233799206\n",
      "\n",
      "starting epoch 279\n",
      "train loss: 0.19843548793417493\n",
      "val loss: 0.45080855286074983\n",
      "\n",
      "starting epoch 280\n",
      "train loss: 0.194778821976513\n",
      "val loss: 0.4558046330976962\n",
      "\n",
      "starting epoch 281\n",
      "train loss: 0.1934946069432186\n",
      "val loss: 0.6320366278714846\n",
      "\n",
      "starting epoch 282\n",
      "train loss: 0.19176677685407023\n",
      "val loss: 0.4965306669454195\n",
      "\n",
      "starting epoch 283\n",
      "train loss: 0.19412681955878344\n",
      "val loss: 0.4841572459386616\n",
      "\n",
      "starting epoch 284\n",
      "train loss: 0.19046011908953492\n",
      "val loss: 0.44200039894502374\n",
      "\n",
      "starting epoch 285\n",
      "train loss: 0.19261880655449243\n",
      "val loss: 0.4931112776056577\n",
      "\n",
      "starting epoch 286\n",
      "train loss: 0.19211716650669675\n",
      "val loss: 0.4582967775135721\n",
      "\n",
      "starting epoch 287\n",
      "train loss: 0.1945074689290565\n",
      "val loss: 0.45647701610408487\n",
      "\n",
      "starting epoch 288\n",
      "train loss: 0.1925814343339486\n",
      "val loss: 0.4185683220658365\n",
      "\n",
      "starting epoch 289\n",
      "train loss: 0.18845889972847987\n",
      "val loss: 0.44752994357392717\n",
      "\n",
      "starting epoch 290\n",
      "train loss: 0.198505963652216\n",
      "val loss: 0.3821848326950204\n",
      "\n",
      "starting epoch 291\n",
      "train loss: 0.18812709967905988\n",
      "val loss: 0.38155008899944104\n",
      "\n",
      "starting epoch 292\n",
      "train loss: 0.18805838603610975\n",
      "val loss: 0.4555117526305042\n",
      "\n",
      "starting epoch 293\n",
      "train loss: 0.1850650122533269\n",
      "val loss: 0.38013536722707353\n",
      "\n",
      "starting epoch 294\n",
      "train loss: 0.19710559209677428\n",
      "val loss: 0.4423261640312099\n",
      "\n",
      "starting epoch 295\n",
      "train loss: 0.1920008697807614\n",
      "val loss: 0.3746229009093446\n",
      "\n",
      "starting epoch 296\n",
      "train loss: 0.1839155295779507\n",
      "val loss: 0.4429864561192348\n",
      "\n",
      "starting epoch 297\n",
      "train loss: 0.18745650309625703\n",
      "val loss: 0.48656544520398537\n",
      "\n",
      "starting epoch 298\n",
      "train loss: 0.1915541823239334\n",
      "val loss: 0.4064598063616306\n",
      "\n",
      "starting epoch 299\n",
      "train loss: 0.18593563399618498\n",
      "val loss: 0.48947494600495167\n",
      "\n",
      "starting epoch 300\n",
      "train loss: 0.19113174176900247\n",
      "val loss: 0.3918407824831323\n",
      "\n",
      "starting epoch 301\n",
      "train loss: 0.18268798199852232\n",
      "val loss: 0.6892864931780037\n",
      "\n",
      "starting epoch 302\n",
      "train loss: 0.1852677050110927\n",
      "val loss: 0.6728978836098791\n",
      "\n",
      "starting epoch 303\n",
      "train loss: 0.1821358725967933\n",
      "val loss: 0.414853674497218\n",
      "\n",
      "starting epoch 304\n",
      "train loss: 0.1810864572273212\n",
      "val loss: 0.7599563813870633\n",
      "\n",
      "starting epoch 305\n",
      "train loss: 0.18102878210477932\n",
      "val loss: 0.7347352879205921\n",
      "\n",
      "starting epoch 306\n",
      "train loss: 0.18451558335129142\n",
      "val loss: 0.6496673501229093\n",
      "\n",
      "starting epoch 307\n",
      "train loss: 0.18532194239862604\n",
      "val loss: 0.6663735038220022\n",
      "\n",
      "starting epoch 308\n",
      "train loss: 0.18273472745847535\n",
      "val loss: 0.3750036634546822\n",
      "\n",
      "starting epoch 309\n",
      "train loss: 0.21492849291837363\n",
      "val loss: 0.5540689934708506\n",
      "\n",
      "starting epoch 310\n",
      "train loss: 0.18472681391845208\n",
      "val loss: 0.46739166809896127\n",
      "\n",
      "starting epoch 311\n",
      "train loss: 0.1793239065677228\n",
      "val loss: 0.4266893103847109\n",
      "\n",
      "starting epoch 312\n",
      "train loss: 0.18766672448420635\n",
      "val loss: 0.4885497902679712\n",
      "\n",
      "starting epoch 313\n",
      "train loss: 0.18008794353894156\n",
      "val loss: 0.4488140907334089\n",
      "\n",
      "starting epoch 314\n",
      "train loss: 0.1730994001844908\n",
      "val loss: 0.7196058022105967\n",
      "\n",
      "starting epoch 315\n",
      "train loss: 0.18387162794835135\n",
      "val loss: 0.4851749862557698\n",
      "\n",
      "starting epoch 316\n",
      "train loss: 0.17598470160503374\n",
      "val loss: 0.44009820387247944\n",
      "\n",
      "starting epoch 317\n",
      "train loss: 0.1721764068986065\n",
      "val loss: 0.4263293291564386\n",
      "\n",
      "starting epoch 318\n",
      "train loss: 0.17624964623472958\n",
      "val loss: 0.6999470773673313\n",
      "\n",
      "starting epoch 319\n",
      "train loss: 0.17085080896515553\n",
      "val loss: 0.37972454023871627\n",
      "\n",
      "starting epoch 320\n",
      "train loss: 0.1714628024985606\n",
      "val loss: 0.37195164495458316\n",
      "\n",
      "starting epoch 321\n",
      "train loss: 0.1748065118692338\n",
      "val loss: 0.4887343832317278\n",
      "\n",
      "starting epoch 322\n",
      "train loss: 0.18182013698284666\n",
      "val loss: 0.6411741465028968\n",
      "\n",
      "starting epoch 323\n",
      "train loss: 0.17371735419017975\n",
      "val loss: 0.4715655284458753\n",
      "\n",
      "starting epoch 324\n",
      "train loss: 0.17482291232404798\n",
      "val loss: 0.6101191168326769\n",
      "\n",
      "starting epoch 325\n",
      "train loss: 0.16891239091776175\n",
      "val loss: 0.7228595217446186\n",
      "\n",
      "starting epoch 326\n",
      "train loss: 0.17176625848430646\n",
      "val loss: 0.7001969234336025\n",
      "\n",
      "starting epoch 327\n",
      "train loss: 0.17545606985661827\n",
      "val loss: 0.4002090438270514\n",
      "\n",
      "starting epoch 328\n",
      "train loss: 0.16538707795239896\n",
      "val loss: 0.4470304903936477\n",
      "\n",
      "starting epoch 329\n",
      "train loss: 0.1690944809663319\n",
      "val loss: 0.6992999503657157\n",
      "\n",
      "starting epoch 330\n",
      "train loss: 0.16552599341168645\n",
      "val loss: 0.401301949823474\n",
      "\n",
      "starting epoch 331\n",
      "train loss: 0.17002460124299934\n",
      "val loss: 0.679516197660681\n",
      "\n",
      "starting epoch 332\n",
      "train loss: 0.16630981248502574\n",
      "val loss: 0.5315043838147265\n",
      "\n",
      "starting epoch 333\n",
      "train loss: 0.16445489677000882\n",
      "val loss: 0.37640344380402097\n",
      "\n",
      "starting epoch 334\n",
      "train loss: 0.16873741902130202\n",
      "val loss: 0.43345041870714557\n",
      "\n",
      "starting epoch 335\n",
      "train loss: 0.1650913698610534\n",
      "val loss: 0.432270777778461\n",
      "\n",
      "starting epoch 336\n",
      "train loss: 0.1623076501251249\n",
      "val loss: 0.44511589477654767\n",
      "\n",
      "starting epoch 337\n",
      "train loss: 0.17029537765045166\n",
      "val loss: 0.3674969635331174\n",
      "\n",
      "starting epoch 338\n",
      "train loss: 0.17010237649424712\n",
      "val loss: 0.42542499089760016\n",
      "\n",
      "starting epoch 339\n",
      "train loss: 0.1654619963340063\n",
      "val loss: 0.4907774272551498\n",
      "\n",
      "starting epoch 340\n",
      "train loss: 0.16586819170498204\n",
      "val loss: 0.43232087792844337\n",
      "\n",
      "starting epoch 341\n",
      "train loss: 0.16228680284300528\n",
      "val loss: 0.39129388656847003\n",
      "\n",
      "starting epoch 342\n",
      "train loss: 0.1650460669141312\n",
      "val loss: 0.4726485697093401\n",
      "\n",
      "starting epoch 343\n",
      "train loss: 0.15718957224144184\n",
      "val loss: 0.46674755145691815\n",
      "\n",
      "starting epoch 344\n",
      "train loss: 0.16334343254973333\n",
      "val loss: 0.623201155557833\n",
      "\n",
      "starting epoch 345\n",
      "train loss: 0.15811149029424865\n",
      "val loss: 0.49556019508096444\n",
      "\n",
      "starting epoch 346\n",
      "train loss: 0.1582558647896372\n",
      "val loss: 0.4813765404888407\n",
      "\n",
      "starting epoch 347\n",
      "train loss: 0.16006377025850485\n",
      "val loss: 0.6642970860922961\n",
      "\n",
      "starting epoch 348\n",
      "train loss: 0.15919834519716738\n",
      "val loss: 0.6621542730602691\n",
      "\n",
      "starting epoch 349\n",
      "train loss: 0.1560071581467714\n",
      "val loss: 0.47442038834390826\n",
      "\n",
      "starting epoch 350\n",
      "train loss: 0.15787870495423845\n",
      "val loss: 0.6469479710528429\n",
      "\n",
      "starting epoch 351\n",
      "train loss: 0.1575752838576577\n",
      "val loss: 0.5415758349533394\n",
      "\n",
      "starting epoch 352\n",
      "train loss: 0.15484128185418666\n",
      "val loss: 0.42283236314479705\n",
      "\n",
      "starting epoch 353\n",
      "train loss: 0.15505836903936013\n",
      "val loss: 0.43547360972961285\n",
      "\n",
      "starting epoch 354\n",
      "train loss: 0.15541495450823628\n",
      "val loss: 0.7002341032210565\n",
      "\n",
      "starting epoch 355\n",
      "train loss: 0.16096917344421932\n",
      "val loss: 0.3734132372878451\n",
      "\n",
      "starting epoch 356\n",
      "train loss: 0.15182351115607598\n",
      "val loss: 0.5176215235617808\n",
      "\n",
      "starting epoch 357\n",
      "train loss: 0.1557163144281957\n",
      "val loss: 0.4664721311807961\n",
      "\n",
      "starting epoch 358\n",
      "train loss: 0.15159765065956537\n",
      "val loss: 0.7054401932459219\n",
      "\n",
      "starting epoch 359\n",
      "train loss: 0.1570077293200534\n",
      "val loss: 0.7766470578626982\n",
      "\n",
      "starting epoch 360\n",
      "train loss: 0.14914805875760723\n",
      "val loss: 0.6310161315189338\n",
      "\n",
      "starting epoch 361\n",
      "train loss: 0.15125702652265763\n",
      "val loss: 0.44666512412201786\n",
      "\n",
      "starting epoch 362\n",
      "train loss: 0.15124780077105168\n",
      "val loss: 0.4139147327776451\n",
      "\n",
      "starting epoch 363\n",
      "train loss: 0.15916311082497664\n",
      "val loss: 0.41699489365312337\n",
      "\n",
      "starting epoch 364\n",
      "train loss: 0.15239482991841125\n",
      "val loss: 0.43590635041960535\n",
      "\n",
      "starting epoch 365\n",
      "train loss: 0.1477206326902559\n",
      "val loss: 0.42830008408088904\n",
      "\n",
      "starting epoch 366\n",
      "train loss: 0.15385129956559435\n",
      "val loss: 0.40938169977340916\n",
      "\n",
      "starting epoch 367\n",
      "train loss: 0.14769669917211814\n",
      "val loss: 0.6829353166326051\n",
      "\n",
      "starting epoch 368\n",
      "train loss: 0.1516905190035435\n",
      "val loss: 0.35582235413091406\n",
      "\n",
      "starting epoch 369\n",
      "train loss: 0.151324055772347\n",
      "val loss: 0.7154604128292136\n",
      "\n",
      "starting epoch 370\n",
      "train loss: 0.15306622870401582\n",
      "val loss: 0.42246786784026696\n",
      "\n",
      "starting epoch 371\n",
      "train loss: 0.14579003520758743\n",
      "val loss: 0.6964241254976071\n",
      "\n",
      "starting epoch 372\n",
      "train loss: 0.14933425518839594\n",
      "val loss: 0.3892739249124875\n",
      "\n",
      "starting epoch 373\n",
      "train loss: 0.14421867921744835\n",
      "val loss: 0.43956138409215934\n",
      "\n",
      "starting epoch 374\n",
      "train loss: 0.14613170194780037\n",
      "val loss: 0.38430861428154545\n",
      "\n",
      "starting epoch 375\n",
      "train loss: 0.14633208567826597\n",
      "val loss: 0.6975962236501321\n",
      "\n",
      "starting epoch 376\n",
      "train loss: 0.14502982564335237\n",
      "val loss: 0.4949667274874727\n",
      "\n",
      "starting epoch 377\n",
      "train loss: 0.14360545775061329\n",
      "val loss: 0.4640844346399867\n",
      "\n",
      "starting epoch 378\n",
      "train loss: 0.14427495976358506\n",
      "val loss: 0.36904826001433694\n",
      "\n",
      "starting epoch 379\n",
      "train loss: 0.1467954624805712\n",
      "val loss: 0.6129892971834399\n",
      "\n",
      "starting epoch 380\n",
      "train loss: 0.13911486443772064\n",
      "val loss: 0.4594199688594689\n",
      "\n",
      "starting epoch 381\n",
      "train loss: 0.14266034674805134\n",
      "val loss: 0.6877247263761932\n",
      "\n",
      "starting epoch 382\n",
      "train loss: 0.14099379118997518\n",
      "val loss: 0.3675782571841151\n",
      "\n",
      "starting epoch 383\n",
      "train loss: 0.13953875082511724\n",
      "val loss: 0.6877129293167186\n",
      "\n",
      "starting epoch 384\n",
      "train loss: 0.1406201173603176\n",
      "val loss: 0.7330484080435751\n",
      "\n",
      "starting epoch 385\n",
      "train loss: 0.14203288099021233\n",
      "val loss: 0.41207378947525863\n",
      "\n",
      "starting epoch 386\n",
      "train loss: 0.13856179685202846\n",
      "val loss: 0.3716809552562428\n",
      "\n",
      "starting epoch 387\n",
      "train loss: 0.13984913905972646\n",
      "val loss: 0.6698130697265006\n",
      "\n",
      "starting epoch 388\n",
      "train loss: 0.13947116292810252\n",
      "val loss: 0.40672237780840376\n",
      "\n",
      "starting epoch 389\n",
      "train loss: 0.13924806100279394\n",
      "val loss: 0.4020303431621902\n",
      "\n",
      "starting epoch 390\n",
      "train loss: 0.13922571228276057\n",
      "val loss: 0.41137739338078994\n",
      "\n",
      "starting epoch 391\n",
      "train loss: 0.13672374310787466\n",
      "val loss: 0.7362331121893423\n",
      "\n",
      "starting epoch 392\n",
      "train loss: 0.13765502954412262\n",
      "val loss: 0.47518211847912517\n",
      "\n",
      "starting epoch 393\n",
      "train loss: 0.1371669673726177\n",
      "val loss: 0.49182069636276904\n",
      "\n",
      "starting epoch 394\n",
      "train loss: 0.14079265266502902\n",
      "val loss: 0.4749575147352339\n",
      "\n",
      "starting epoch 395\n",
      "train loss: 0.14129371647917116\n",
      "val loss: 0.6926107832254779\n",
      "\n",
      "starting epoch 396\n",
      "train loss: 0.13659136528909244\n",
      "val loss: 0.4130214816509521\n",
      "\n",
      "starting epoch 397\n",
      "train loss: 0.14242253016993459\n",
      "val loss: 0.4103770400172165\n",
      "\n",
      "starting epoch 398\n",
      "train loss: 0.1368132213949468\n",
      "val loss: 0.774812742895477\n",
      "\n",
      "starting epoch 399\n",
      "train loss: 0.13635612062713182\n",
      "val loss: 0.719802757303788\n",
      "\n",
      "starting epoch 400\n",
      "train loss: 0.13373306180740058\n",
      "val loss: 0.7284347739966641\n",
      "\n",
      "starting epoch 401\n",
      "train loss: 0.13333812837836184\n",
      "val loss: 0.5026365324654749\n",
      "\n",
      "starting epoch 402\n",
      "train loss: 0.13348005246282757\n",
      "val loss: 0.4814515939934712\n",
      "\n",
      "starting epoch 403\n",
      "train loss: 0.14078379125809404\n",
      "val loss: 0.6483364279991033\n",
      "\n",
      "starting epoch 404\n",
      "train loss: 0.13569945981933165\n",
      "val loss: 0.42339491912514715\n",
      "\n",
      "starting epoch 405\n",
      "train loss: 0.13809990774641884\n",
      "val loss: 0.6788251809032022\n",
      "\n",
      "starting epoch 406\n",
      "train loss: 0.13515878501794504\n",
      "val loss: 0.43642557981076857\n",
      "\n",
      "starting epoch 407\n",
      "train loss: 0.13133256873572044\n",
      "val loss: 0.3792071170324056\n",
      "\n",
      "starting epoch 408\n",
      "train loss: 0.13151060610283385\n",
      "val loss: 0.7000436518596174\n",
      "\n",
      "starting epoch 409\n",
      "train loss: 0.13251354747766306\n",
      "val loss: 0.4303097907689745\n",
      "\n",
      "starting epoch 410\n",
      "train loss: 0.134680524752941\n",
      "val loss: 0.4657323779353575\n",
      "\n",
      "starting epoch 411\n",
      "train loss: 0.1312636288116397\n",
      "val loss: 0.6570562474064008\n",
      "\n",
      "starting epoch 412\n",
      "train loss: 0.12812792354924987\n",
      "val loss: 0.5172009866984766\n",
      "\n",
      "starting epoch 413\n",
      "train loss: 0.13118714954918256\n",
      "val loss: 0.677982391802139\n",
      "\n",
      "starting epoch 414\n",
      "train loss: 0.12904428861404918\n",
      "val loss: 0.708106420976996\n",
      "\n",
      "starting epoch 415\n",
      "train loss: 0.13635786639511446\n",
      "val loss: 0.5001649703286019\n",
      "\n",
      "starting epoch 416\n",
      "train loss: 0.13271577693571684\n",
      "val loss: 0.4204744588082845\n",
      "\n",
      "starting epoch 417\n",
      "train loss: 0.12849817536626693\n",
      "val loss: 0.36974599579408035\n",
      "\n",
      "starting epoch 418\n",
      "train loss: 0.12671941618992466\n",
      "val loss: 0.7679862335047373\n",
      "\n",
      "starting epoch 419\n",
      "train loss: 0.13016549958949658\n",
      "val loss: 0.42777663771028557\n",
      "\n",
      "starting epoch 420\n",
      "train loss: 0.12418629031588177\n",
      "val loss: 0.6385261305103197\n",
      "\n",
      "starting epoch 421\n",
      "train loss: 0.12748921223400375\n",
      "val loss: 0.7759908128472123\n",
      "\n",
      "starting epoch 422\n",
      "train loss: 0.12728393534095914\n",
      "val loss: 0.3826409467873074\n",
      "\n",
      "starting epoch 423\n",
      "train loss: 0.1366926079307022\n",
      "val loss: 0.36922087131155196\n",
      "\n",
      "starting epoch 424\n",
      "train loss: 0.13075770130295347\n",
      "val loss: 0.46313134712754533\n",
      "\n",
      "starting epoch 425\n",
      "train loss: 0.12666141358975253\n",
      "val loss: 0.7308726806051092\n",
      "\n",
      "starting epoch 426\n",
      "train loss: 0.1240439335505763\n",
      "val loss: 0.7232150067324865\n",
      "\n",
      "starting epoch 427\n",
      "train loss: 0.12322866788102745\n",
      "val loss: 0.7262859025427566\n",
      "\n",
      "starting epoch 428\n",
      "train loss: 0.127241014959774\n",
      "val loss: 0.7418406566826613\n",
      "\n",
      "starting epoch 429\n",
      "train loss: 0.12035255708865511\n",
      "val loss: 0.3891603271229459\n",
      "\n",
      "starting epoch 430\n",
      "train loss: 0.12318764735550643\n",
      "val loss: 0.72472383611207\n",
      "\n",
      "starting epoch 431\n",
      "train loss: 0.12267696965016009\n",
      "val loss: 0.4787697862543985\n",
      "\n",
      "starting epoch 432\n",
      "train loss: 0.1261164939394771\n",
      "val loss: 0.35934440591274097\n",
      "\n",
      "starting epoch 433\n",
      "train loss: 0.1245249781492102\n",
      "val loss: 0.7233458694256735\n",
      "\n",
      "starting epoch 434\n",
      "train loss: 0.12296440001203619\n",
      "val loss: 0.696670267599554\n",
      "\n",
      "starting epoch 435\n",
      "train loss: 0.12200653838650481\n",
      "val loss: 0.37518083368499855\n",
      "\n",
      "starting epoch 436\n",
      "train loss: 0.12539530463460544\n",
      "val loss: 0.7025745654838944\n",
      "\n",
      "starting epoch 437\n",
      "train loss: 0.12156994231361987\n",
      "val loss: 0.43062457190166314\n",
      "\n",
      "starting epoch 438\n",
      "train loss: 0.1199747587963004\n",
      "val loss: 0.39632000012583024\n",
      "\n",
      "starting epoch 439\n",
      "train loss: 0.11880456295346378\n",
      "val loss: 0.4489225021679478\n",
      "\n",
      "starting epoch 440\n",
      "train loss: 0.12106490295007107\n",
      "val loss: 0.46076151068106797\n",
      "\n",
      "starting epoch 441\n",
      "train loss: 0.12515079415955394\n",
      "val loss: 0.6535547399457782\n",
      "\n",
      "starting epoch 442\n",
      "train loss: 0.11825515978671312\n",
      "val loss: 0.41401320510831413\n",
      "\n",
      "starting epoch 443\n",
      "train loss: 0.1187873826491255\n",
      "val loss: 0.526808322141829\n",
      "\n",
      "starting epoch 444\n",
      "train loss: 0.1162512327189577\n",
      "val loss: 0.743078830449534\n",
      "\n",
      "starting epoch 445\n",
      "train loss: 0.11866739625241296\n",
      "val loss: 0.4119791263389954\n",
      "\n",
      "starting epoch 446\n",
      "train loss: 0.11791568575821336\n",
      "val loss: 0.7762131729629539\n",
      "\n",
      "starting epoch 447\n",
      "train loss: 0.1201583435487048\n",
      "val loss: 0.6736518376399213\n",
      "\n",
      "starting epoch 448\n",
      "train loss: 0.12897361991973127\n",
      "val loss: 0.4573231962144371\n",
      "\n",
      "starting epoch 449\n",
      "train loss: 0.124688768523843\n",
      "val loss: 0.6440991070378541\n",
      "\n",
      "starting epoch 450\n",
      "train loss: 0.11814966515920808\n",
      "val loss: 0.5082246617777375\n",
      "\n",
      "starting epoch 451\n",
      "train loss: 0.11970872232499705\n",
      "val loss: 0.3749845327628324\n",
      "\n",
      "starting epoch 452\n",
      "train loss: 0.11746868989949029\n",
      "val loss: 0.4169402456197959\n",
      "\n",
      "starting epoch 453\n",
      "train loss: 0.11326603948470684\n",
      "val loss: 0.3805874950254298\n",
      "\n",
      "starting epoch 454\n",
      "train loss: 0.11280172443010843\n",
      "val loss: 0.7268763212596345\n",
      "\n",
      "starting epoch 455\n",
      "train loss: 0.11633148498522275\n",
      "val loss: 0.8513312820463358\n",
      "\n",
      "starting epoch 456\n",
      "train loss: 0.11474420924820188\n",
      "val loss: 0.37436490028246283\n",
      "\n",
      "starting epoch 457\n",
      "train loss: 0.11819066822147754\n",
      "val loss: 0.4695856881439571\n",
      "\n",
      "starting epoch 458\n",
      "train loss: 0.11268697183988083\n",
      "val loss: 0.45283004078976385\n",
      "\n",
      "starting epoch 459\n",
      "train loss: 0.11166441450776354\n",
      "val loss: 0.4053498376075768\n",
      "\n",
      "starting epoch 460\n",
      "train loss: 0.11349956046944938\n",
      "val loss: 0.4364385359165158\n",
      "\n",
      "starting epoch 461\n",
      "train loss: 0.11015931308857556\n",
      "val loss: 0.3977377378381174\n",
      "\n",
      "starting epoch 462\n",
      "train loss: 0.110725178129554\n",
      "val loss: 0.8032835750012811\n",
      "\n",
      "starting epoch 463\n",
      "train loss: 0.1097841178623766\n",
      "val loss: 0.41015802719054545\n",
      "\n",
      "starting epoch 464\n",
      "train loss: 0.111641359720602\n",
      "val loss: 0.4407595495707167\n",
      "\n",
      "starting epoch 465\n",
      "train loss: 0.11011190972533495\n",
      "val loss: 0.378649551675583\n",
      "\n",
      "starting epoch 466\n",
      "train loss: 0.10835770267781349\n",
      "val loss: 0.41115796493823886\n",
      "\n",
      "starting epoch 467\n",
      "train loss: 0.10927952959580338\n",
      "val loss: 0.4013921183286806\n",
      "\n",
      "starting epoch 468\n",
      "train loss: 0.10835641816438195\n",
      "val loss: 0.7144027265364384\n",
      "\n",
      "starting epoch 469\n",
      "train loss: 0.108704687143208\n",
      "val loss: 0.48799781384171087\n",
      "\n",
      "starting epoch 470\n",
      "train loss: 0.11285352045331416\n",
      "val loss: 0.44746754104751496\n",
      "\n",
      "starting epoch 471\n",
      "train loss: 0.11214222403557031\n",
      "val loss: 0.43193142175172955\n",
      "\n",
      "starting epoch 472\n",
      "train loss: 0.10840560284962264\n",
      "val loss: 0.4375210478875901\n",
      "\n",
      "starting epoch 473\n",
      "train loss: 0.11338529027155012\n",
      "val loss: 0.4645440444843105\n",
      "\n",
      "starting epoch 474\n",
      "train loss: 0.10831449893685756\n",
      "val loss: 0.7059228778119593\n",
      "\n",
      "starting epoch 475\n",
      "train loss: 0.10585276960351238\n",
      "val loss: 0.7621331596856222\n",
      "\n",
      "starting epoch 476\n",
      "train loss: 0.10609034441580004\n",
      "val loss: 0.8016024703614767\n",
      "\n",
      "starting epoch 477\n",
      "train loss: 0.1051371051373929\n",
      "val loss: 0.7610027178941\n",
      "\n",
      "starting epoch 478\n",
      "train loss: 0.10507833278250618\n",
      "val loss: 0.4710080548661993\n",
      "\n",
      "starting epoch 479\n",
      "train loss: 0.1058394717289239\n",
      "val loss: 0.43560063763437845\n",
      "\n",
      "starting epoch 480\n",
      "train loss: 0.10828275467726764\n",
      "val loss: 0.4311939980432669\n",
      "\n",
      "starting epoch 481\n",
      "train loss: 0.10486594531877116\n",
      "val loss: 0.8198118858187651\n",
      "\n",
      "starting epoch 482\n",
      "train loss: 0.10449103456608858\n",
      "val loss: 0.7161807968857266\n",
      "\n",
      "starting epoch 483\n",
      "train loss: 0.10590831287107813\n",
      "val loss: 0.48342077843615316\n",
      "\n",
      "starting epoch 484\n",
      "train loss: 0.1146598425210608\n",
      "val loss: 0.7308197115413593\n",
      "\n",
      "starting epoch 485\n",
      "train loss: 0.12693584582596554\n",
      "val loss: 0.4820856609790086\n",
      "\n",
      "starting epoch 486\n",
      "train loss: 0.14049012649769893\n",
      "val loss: 0.4181543518196681\n",
      "\n",
      "starting epoch 487\n",
      "train loss: 0.10710184468483559\n",
      "val loss: 0.5118480829635752\n",
      "\n",
      "starting epoch 488\n",
      "train loss: 0.10979947688751165\n",
      "val loss: 0.3851094542058932\n",
      "\n",
      "starting epoch 489\n",
      "train loss: 0.10129651787138445\n",
      "val loss: 0.4929012538117858\n",
      "\n",
      "starting epoch 490\n",
      "train loss: 0.10213827488737699\n",
      "val loss: 0.45034620378907875\n",
      "\n",
      "starting epoch 491\n",
      "train loss: 0.10267612005191082\n",
      "val loss: 0.3919167747364329\n",
      "\n",
      "starting epoch 492\n",
      "train loss: 0.10085407960179822\n",
      "val loss: 0.8119118322711336\n",
      "\n",
      "starting epoch 493\n",
      "train loss: 0.10093400288655072\n",
      "val loss: 0.4525824542894097\n",
      "\n",
      "starting epoch 494\n",
      "train loss: 0.10101511255217237\n",
      "val loss: 0.3802831298287874\n",
      "\n",
      "starting epoch 495\n",
      "train loss: 0.10046982381244465\n",
      "val loss: 0.38242415007700475\n",
      "\n",
      "starting epoch 496\n",
      "train loss: 0.09910485140756374\n",
      "val loss: 0.39156533754759965\n",
      "\n",
      "starting epoch 497\n",
      "train loss: 0.10531010531119978\n",
      "val loss: 0.43277445969847944\n",
      "\n",
      "starting epoch 498\n",
      "train loss: 0.10111552355699367\n",
      "val loss: 0.45850508060113\n",
      "\n",
      "starting epoch 499\n",
      "train loss: 0.10168599294771805\n",
      "val loss: 0.42930058252245995\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "# per ogni epoca:\n",
    "#     per ogni sample nel training set:\n",
    "#         azzera gradienti dell'optimizer\n",
    "#         metti un po' di noise nei dati\n",
    "#         dividi dati del sample in input e target\n",
    "#         ottieni output dal modello\n",
    "#         se sample è vivo e output > target:\n",
    "#             se output > 5 anni:\n",
    "#                 calcola loss tra output e [5 anni]\n",
    "#             else:\n",
    "#                 non calcolare la loss, passa al prossimo sample senza fare back-propagation\n",
    "#         else:\n",
    "#             calcola loss tra output e target\n",
    "#         back-propagation\n",
    "#         optimizer.step()\n",
    "#     ripeti sul validation set, ma senza fare back-propagation\n",
    "#     salva il modello\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(500):\n",
    "\n",
    "    print(f\"\\nstarting epoch {epoch}\")\n",
    "\n",
    "    epoch_train_loss = []\n",
    "\n",
    "    my_model.train()\n",
    "\n",
    "    dataset.split = \"train\"\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "\n",
    "        #x_train = inputs.reshape([inputs.shape[0], 1, inputs.shape[1]])\n",
    "        #y_train = targets.reshape([targets.shape[0], 1, targets.shape[1]])\n",
    "        \n",
    "        my_optim.zero_grad()\n",
    "\n",
    "        #sample = RNA_train[s_name]\n",
    "        \n",
    "        output = my_model(inputs)\n",
    "\n",
    "        sample_loss = my_loss(output, targets)\n",
    "    \n",
    "        sample_loss.backward()\n",
    "        my_optim.step()\n",
    "\n",
    "        epoch_train_loss.append(sample_loss.item())\n",
    "\n",
    "    mean_train_loss = pd.Series(epoch_train_loss).mean()\n",
    "\n",
    "    print(f\"train loss: {mean_train_loss}\")\n",
    "    train_losses.append(mean_train_loss)\n",
    "\n",
    "    epoch_val_loss = []\n",
    "\n",
    "    my_model.eval()\n",
    "\n",
    "    dataset.split = \"val\"\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        \n",
    "        x_val = inputs.reshape([inputs.shape[0], 1, inputs.shape[1]])\n",
    "        \n",
    "        output = my_model(inputs)\n",
    "\n",
    "        sample_loss = my_loss(output, targets)\n",
    "    \n",
    "        epoch_val_loss.append(sample_loss.item())\n",
    "\n",
    "    mean_val_loss = pd.Series(epoch_val_loss).mean()\n",
    "\n",
    "    print(f\"val loss: {mean_val_loss}\")\n",
    "\n",
    "    val_losses.append(mean_val_loss)\n",
    "\n",
    "    if epoch == 0 or mean_val_loss < minor_val_loss:\n",
    "\n",
    "        minor_val_loss = mean_val_loss\n",
    "\n",
    "        checkpoint = { \n",
    "            'epoch': epoch,\n",
    "            'model': my_model.state_dict(),\n",
    "            'optimizer': my_optim.state_dict()}\n",
    "            # \"scheduler\": my_scheduler}\n",
    "\n",
    "        save(checkpoint,f\"RNA_class_epochs/{epoch}.pth\")\n",
    "\n",
    "print(\"finished!\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAABiyklEQVR4nO2dd5gV1dnAf+/dzsJSl96liaigCNgI9t5i7y2a5NMYTdRYojFq1MR0NbHX2HtDERGwgTRB6b0sdVnYZVnYfr4/zszeuXPnll22Ae/vee5z78ycmTkzd+a85y3nPWKMQVEURVFiEWrqCiiKoijNGxUUiqIoSlxUUCiKoihxUUGhKIqixEUFhaIoihIXFRSKoihKXFRQ7GWIyEoRObaBjn2kiCzyLA8UkdkiUiwiN4jI4yJyVwOc9w4Rebq+j9tQiMjzInJ/kmUb7P/ynCPp+7e73Ova3ONaHvceEfmf87uniGwXkZREZet4rnkiMqau+9cnKiioeRl3On/6Bucha+nZ/ryIGBEZ4VnXT0SiBqE4ZStFpEtj1d93/hwR+aeIrHauZ5mz3KGhz22M+coYM9Cz6lZgojGmlTHm38aYXxhj7tuVc4jIGBHJ8533AWPMz3bluDHOdYXzv//Dt/4MZ/3z9X3OZBGRT5z/d7uIVIhIuWf58docqzb3r6HudWMhIqNEpMT7fnu2fS8i1yd7LGPMamNMS2NMVT3UK0qwGWP2M8ZM2tVj1wcqKMKcZoxpCQwFhgG3+7ZvAeL2UEQkGzgbKAIuaYA6xkVE0oEJwH7AiUAOcChQAIyIs2tD0QuY1wTnrU+WAeeJSKpn3eXA4iaqDwDGmJOcRqol8DLwF3fZGPMLt5yv3ns9xpipQB5wjne9iAwBBgOvNkW9mjsqKHwYYzYA47ACw8sLwAEi8pM4u58NFAL3YhuTGkRkhIjMEJFtIrJRRP7u2TZKRL4VkUIRmeNVN0Wkj4hMdsw340Xk0Tjq7GVAT+AsY8x8Y0y1MWaTMeY+Y8xYf2GnTlOc8653jp3ubBMR+YeIbHLq/KPzMiEiJ4vIfKdOa0XkZmd9TW9fRL4AjgIedXq5A/y9JqdnPts5/jIROdFZf6WILHCOv1xEfu6szwY+Abp6es9d/Sq+iJzuqO2FIjJJRPb1bFspIjeLyA8iUiQir4tIZpz/dAPwI3CCs3874DDgA9+9jHfOYSIyy7me14FM376nOveh0HkODohTn4Q42s51IrIEWOKs+5eIrHHu9UwROdJT3mtO6e3sf7lYrXSziNxZx7JZIvKCiGx1/s9bxacN+uqdqI5viMiLzn2cJyLDk73HPl7AviteLgPGGmMK4tXDV1/3+lOd5T7ieVeBDr7yb4q1WBSJyJcisp+z/lrgYuBW55n+0FlfY3YUkQyxloF1zuefIpLhbBsjInki8lux7+t6EbkyzvXXGhUUPkSkO3ASsNS3aQfwAPCnOLtfju2RvAYMEpGDPdv+BfzLGJMD7AO84ZyvG/AxVltpB9wMvC0iuc5+rwAzsQ/dffgEkI9jgU+NMdsTXKZLFXCTc+xDgWOA/3O2HQ+MBgYArYHzsJoJwDPAz40xrYAhwBf+Axtjjga+Aq53erkRPXCxZrwXgVuANs65VjqbNwGnYjWiK4F/iMhBxpgS7H+zztN7Xuc77gDsf3AjkAuMBT4URwA6nIfVuPoABwBXJLhPLxJuWC4A3gfKkjmnc973gJew/++b2A6Fu+8w4Fng50B74AngA7cR2AXOBEZie8kA07Gdn3bYZ+rNBALyCGAg9pm42yv4alH2D0BvoC9wHIm17ER1PB37brXBCupHoUaTfo8Y9ziAl4DRItLD2T8EXIQVIMnUIxaJ3tVPgP5AR2AWVhPEGPMkkVrhaQHHvhMY5dTrQKyF4Pee7Z2x72k34GrgMRFpm0Sdk0IFRZj3RKQYWINtqP4QUOYJoKeInOTfICI9sT3oV4wxG7EmIG+vpQLoJyIdjDHbHRUY7Msz1hgz1tEAxgMzgJOdYx4C3GWMKTPGfAl8GOca2gPrk71gY8xMY8xUY0ylMWalc32uxlQBtAIGAWKMWWCMWe/ZNlhEcowxW40xs5I9p4ergWeNMeOd615rjFno1OtjY8wyY5kMfAYE9uoCOB/42DluBfBXIAurBbj82xizzhizBXs/hyY45rvAGBFpjf1PX6zFOUcBacA/jTEVxpi3sA2Ry7XAE8aY74wxVcaYF7BCaFSS1xuLB40xW4wxOwGMMf8zxhQ4//XfgAxs4x6LPxpjdhpj5gBzsI1TbcueBzzgPCN5wL/jVTiJOn7tvCdV2MbePU+ie+w/zxpgEnCps+oY51wfJ1mPKJJ5V40xzxpjio0xZcA9wIHOM5UMFwP3OhaCfOCPnvqDfSfvda5/LLA9UZ1rgwqKMGc6PeQx2MYxyvnr/MH3OR8/lwILjDGzneWXgYtEJM1ZvhrbO18oItNF5FRnfS/gXMfsUCgihdgeWhegK7DV6Um7rIpzDQXOfkkh1hz0kaMOb8NqTB2ca/0C22N7DNgkIk+KSI6z69nAycAqR9U+NNlzeuiBtf8H1eskEZkqIluc+3EyAf9HDLriuUfGmGqs8O/mKbPB83sHEOXY9OI0th9je3DtjTHf1OKcXYG1xkRk3/T+h72A3/r+/x7OfrvCGu+CWHPbAsfsUYjtfca7p7W5R7HKdvXVI6JOfpKoo/88mY7ZJ9E9DuIFwg3tpcBrjpCvy72CBO+qiKSIyENiTazbCGvPdXqund/eZ6TAGFPpWU74XNcGFRQ+nB7s89heYRDPYVXfn/rWXwb0dRrdDcDfsQ/Byc5xlxhjLsSqnX8G3hJrc18DvGSMaeP5ZBtjHsJqB22dci4941T/c+AEX/l4/BdYCPR3TGJ3AOJuNDZS6WCs+WIA1kyEMWa6MeYM51rewzGj1ZI1WBNcBI7J5W3s/e9kjGmDNeW49UqU7ngdtvF1jyfYhndtHero5UXgt0CQfyjeOdcD3Zx1Lt7/cA3wJ9//38IYs6tO1Zr75NjYb8X28Ns697QIz3/dQKwHunuWe8QquIt1THSPg3gH6C4iR2Hf5Rd2sR6J3tWLgDOw5uHWWJMc1PG5do69LkbZekcFRTD/BI4TkSh125HafwB+565zetT7YO2GQ53PEKzN8jKnzCUikuv0NgudXauxDc9pInKC0+vIdJxT3Y0xq7BmqD869u4jgCD7pctL2IbnbREZJCIhEWkvNvb95IDyrYBtwHYRGQT80nNNh4jISEcjKgFKgWqnHheLSGunB7bNuY7a8gxwpYgc49Szm1OHdKyqnw9UOma+4z37bQTax1HZ3wBOcY6bhm3cy4Bv61BHL5OxdvZHannOKUAlcIOIpInIT4mMQHsK+IVzr0VEskXkFBFptYv19dLKqUM+kCoid2P9Pw3NG8DtItJWrC8uXujprtQx0T2Owun5v4Xt+K0yxszYlXok8a62wj4TBUALrPbuZSPWlxOLV4Hfi0iu2FD3uwnutDQIKigCcGyAL2L/jCBeJdIXcDnwvjHmR2PMBveDdWCfKjZS5kRgnohsd9Zf4Nh112B7GndgH8412J67+99chHVKbsEKKL993FvvMmyPZSEwHtuIT8NqNt8F7HKzc/xibIP1umdbjrNuK1bNLQAedrZdCqx0VOhfYO2ntcIYMw3HUY3tsU0GehljioEbsI3MVqd+H3j2W4i9/8sdU01X33EXYf0+jwCbsS/racaY8trW0XdcY4yZ4Pg1/NtintM570+xDvMtWH/GO559ZwDXYM18W7FBFFfsSl0DGAd8ig3pXYUV+nHNQPXEvdhQ1BVYbfctPEEA9VXHRPc4Di9ge+ned2pX7lW8d/VF53hrgfnAVN++z2D9foUi8l7Ase/HCqIfsFF4s0gQrl+fiNGJi3YrROQeoJ8xptHHaSjKriAiv8R2kOKFmCvNENUoFEVpEESki4gc7pgWB2JNcu82db2U2qOjNhVFaSjSsSHXfbB+udeA/zRlhZS6oaYnRVEUJS5qelIURVHiskeanjp06GB69+7d1NVQFEXZbZg5c+ZmY0xu0LY9UlD07t2bGTNmJC6oKIqiACAiMUezq+lJURRFiYsKCkVRFCUuKigURVGUuDSpj0JEnsXOO7DJGDMkYLtg012cjM2GeIWpW0prRVGUuFRUVJCXl0dpaWlTV6VByczMpHv37qSlpSUu7NDUzuznsTluYuUvOgk70Ud/bA6V/zrfiqIo9UpeXh6tWrWid+/eRCai3XMwxlBQUEBeXh59+vRJer8mNT05k3tEJVnzcAbwopOQbSrQRkSSnm9BURQlWUpLS2nfvv0eKyQARIT27dvXWmtq7j6KbkRmbswjcgKaGkTkWrFzUs/Iz89vlMopirJnsScLCZe6XGNzFxRJY4x50hgz3BgzPDc3cMxIQv49YQmTF6uQURRF8dLcBcVaImfF6s6uz1QWk/9OWsbXS1RQKIrS+BQWFvKf/9Q+Z+LJJ59MYWFh/VfIQ3MXFB8Alzkzf40Ciowx6xPtVFdSQkJVXeZqUxRF2UViCYrKysqA0mHGjh1LmzZtGqhWlqYOj30VGAN0EJE87KxQaQDGmMexcyWfjJ31awd2RrQGIyRQrdl0FUVpAm677TaWLVvG0KFDSUtLIzMzk7Zt27Jw4UIWL17MmWeeyZo1aygtLeXXv/411157LRBOWbR9+3ZOOukkjjjiCL799lu6devG+++/T1ZW1i7XrUkFhTHmwgTbDXBdI1XH0ShUUCjK3s4fP5zH/HXb6vWYg7vm8IfT9ou5/aGHHmLu3LnMnj2bSZMmccoppzB37tyaMNZnn32Wdu3asXPnTg455BDOPvts2rdvH3GMJUuW8Oqrr/LUU09x3nnn8fbbb3PJJbs+GWZTj6NoVqSEhCrVKBRFaQaMGDEiYqzDv//9b959104QuGbNGpYsWRIlKPr06cPQoUMBOPjgg1m5cmW91EUFhYeQCNWqUSjKXk+8nn9jkZ2dXfN70qRJfP7550yZMoUWLVowZsyYwLEQGRkZNb9TUlLYuXNnvdSluTuzGxU1PSmK0lS0atWK4uLiwG1FRUW0bduWFi1asHDhQqZOndqodVONwkNI1PSkKErT0L59ew4//HCGDBlCVlYWnTp1qtl24okn8vjjj7PvvvsycOBARo0a1ah1U0HhISWkpidFUZqOV155JXB9RkYGn3zySeA21w/RoUMH5s6dW7P+5ptvrrd6qenJg3VmN3UtFEVRmhcqKDyEBNUoFEVRfKig8KDObEVRlGhUUHhQZ7aiKEo0Kig8qDNbURQlGhUUHnRktqIoSjQqKDyERH0UiqLsHrRs2bLRzqWCwkNKSDR7rKIoig8dcOchRTUKRVGaiNtuu40ePXpw3XU2YfY999xDamoqEydOZOvWrVRUVHD//fdzxhlnNHrdVFB4CIWgWicuUhTlk9tgw4/1e8zO+8NJD8XcfP7553PjjTfWCIo33niDcePGccMNN5CTk8PmzZsZNWoUp59+eqPP7a2CwkNKSKjQKe4URWkChg0bxqZNm1i3bh35+fm0bduWzp07c9NNN/Hll18SCoVYu3YtGzdupHPnzo1aNxUUHtSZrSgKELfn35Cce+65vPXWW2zYsIHzzz+fl19+mfz8fGbOnElaWhq9e/cOTC/e0Kig8KDObEVRmpLzzz+fa665hs2bNzN58mTeeOMNOnbsSFpaGhMnTmTVqlVNUi8VFB7Uma0oSlOy3377UVxcTLdu3ejSpQsXX3wxp512Gvvvvz/Dhw9n0KBBTVIvFRQeQprrSVGUJubHH8NO9A4dOjBlypTActu3b2+sKuk4Ci8poqYnRVEUP00qKETkRBFZJCJLReS2gO1XiEi+iMx2Pj9ryPpo9lhFUZRomsz0JCIpwGPAcUAeMF1EPjDGzPcVfd0Yc31j1CkUElROKMreizGm0ccoNDamDlaTptQoRgBLjTHLjTHlwGtA4w859JAiqEahKHspmZmZFBQU1Kkh3V0wxlBQUEBmZmat9mtKZ3Y3YI1nOQ8YGVDubBEZDSwGbjLGrAkog4hcC1wL0LNnzzpVSJ3ZirL30r17d/Ly8sjPz2/qqjQomZmZdO/evVb7NPeopw+BV40xZSLyc+AF4OiggsaYJ4EnAYYPH16n1l6d2Yqy95KWlkafPn2auhrNkqY0Pa0FeniWuzvrajDGFBhjypzFp4GDG7JC6sxWFEWJpikFxXSgv4j0EZF04ALgA28BEeniWTwdWNCQFQrpyGxFUZQomsz0ZIypFJHrgXFACvCsMWaeiNwLzDDGfADcICKnA5XAFuCKhqyTjsxWFEWJpkl9FMaYscBY37q7Pb9vB25vrPqcuebPlFb3BY5vrFMqiqI0e3RktochWz9niFnS1NVQFEVpVqig8FAZSifNVDR1NRRFUZoVKig8VIXSSaO8qauhKIrSrFBB4aEylE66ahSKoigRqKDwUC3ppKtGoSiKEoEKCg+VoQwyqNijc70oiqLUFhUUHqpT0smgQjPIKoqieFBB4aEqlEGGVOigO0VRFA8qKDxUhdLJoFzTeCiKonhQQeGhOiWDdCpVo1AURfGggsJDdYp1ZlepRqEoilKDCgoP1aF0MqSCatUoFEVRalBB4cFqFOXRpqe1s+Ce1rBxXtNUTFEUpQlRQeGhxvRUWRa5YfYr9nvFl41fKUVRlCZGBYWH6pQMsqWMjv/sATNfCG/YvsF+t+zYNBVTFEVpQlRQeDAp6eGF7/8X/l280X6HYkzfYQwUrQ3epiiKspujgsJDdUpGeKHKk/NpuyMoKmPkgZr1IvxjsPVlKIqi7GGooPASISicLLLV1VC4yllXFt5WXhIuu/Jr+71ZJz1SFGXPQwWFl9TM8G9Xo1jyWfS6V86DB7qG14s4PzSsVlGUPQ8VFB5CaV5B4WgPq78Nr3NNT8u+aLxKKYqiNDEqKDyYlp3CCzsL7XfhGmjZ2f6u8oXN1vgsBEVRlD2VJhUUInKiiCwSkaUiclvA9gwRed3Z/p2I9G7I+lR2HBJeKNtmfRGFq6H9PnZdlc+ZXVbckNVRFEVpFjSZoBCRFOAx4CRgMHChiAz2Fbsa2GqM6Qf8A/hzQ9YptVVu5IrSbVZQtOtjl/1RT2Xb7Lfro6iubMjqKYqiNAlNqVGMAJYaY5YbY8qB14AzfGXOANyRb28Bx4hIg9l5MtNSGFc1PLxi+0Yo2QRtetmIKL/pacGHkaO1K0sbqmqKoihNRlMKim7AGs9ynrMusIwxphIoAtoHHUxErhWRGSIyIz8/v04VykpL4ecVv+Hr4Y/YFUvH2+9OQyA1Ixwy6zL+LnjhtPDyhh+huiq8XFkG4/8AZdvrVB9FUZTmwB7jzDbGPGmMGW6MGZ6bm5t4hwAy0uztKAll2xVz34FQGvQ5ElLSbMMPICnBB5j5PIy7M7w860X45p/w1V/rVB9FUZTmQFMKirVAD89yd2ddYBkRSQVaAwUNVaHMNCsASnAExfrZ0PtwyGgVNj0ZEzneAiK1iPnvhX+7pqhYI7oVRVF2A2IkL2oUpgP9RaQPViBcAFzkK/MBcDkwBTgH+MKYhptVKMsRFEUmO7zywAvtt4Rs/qfWPSE1HSo8I7O9vouSzeHfblUbzq2iKIrS4DSZRuH4HK4HxgELgDeMMfNE5F4ROd0p9gzQXkSWAr8BokJo65O0lBApIWEbWeGV/Y6z3+VOKOykB6I1Cm9a8mqPH8NU128Ft67SNCGKojQ6TalRYIwZC4z1rbvb87sUOLcx65SZGmJbtUcQZDu+8/Id4XXik6/+aKfqKgilUJPSY8WXVtPI7rBrlfvXAfb7nqJdO46iKEot2GOc2fVFZloKOysDrFteTcE/sVGFT1CUO1FOrulpww/w/Km7VrEFH+3a/oqiKHVEBYWPzLQUSiuq4KdPw7WTgwuV+8JdvZlkwTNi2yNw8hfEP/HbP4NvH429/fWL4+/fHKmqhIkPhNOhKIqyW6KCwkdmWoiyimo44FzoOjS4UGUpdNo/vFxaGLndHTdRGx/Fj2/CZ3cmLpcsq6bA8hiCrrFY+BFM/jN8fk/T1kNRlF1CBYWPzLQUdlZUJS446GQ42RkfsXNr5Lb1c2DtTDuXRTL4B/L5qUug13MnwounJy7XkFTsjPxWlL2N/MXw6R11e4ebESoofGS5pqdEpKRBmhMd5TdFvXstPHV0dMqPWJQkGEnuzyGVrABqalyNyu/8V5S9hf+dDVMfg23rmromu4S+wT5aZ6WxpSSJAXIpGWFBAdCqCxx6fWQZv5M7Fu5Uq7HwR1VV+nro3vm6V0+NHADYlBinHiF9zJS9FNcsnZJWt/2L1tp3uonRN9hH7w7ZrCwoobrapyqe+GfY3xOpm5oBqR5BMeqXMOKayH22b4h9ooJl4QZ9+6b4lYqKsvIIijXT7Xzds1+1v589ASb/Jf7xGgvXpKYahbK34maY9nbeijfEN0UZE7YaPPkT+04H8e4vYOp/66eeCdA32EfvDtmUVlSzsdjXix/1Czj7aXAnN0pJj9QocgdBeqvIfbaujFx2H44ty+GRg2DSQzY/1Ae/sutj5ZDyaxTeKKttefZ74UdhzWT9nFiXB8snwdf/aBybqVvPWNelKHsLrna9cR78bSBMfzp22bevhge729+uWTooDdCcV+HTBh2DXIMKCh99O9j0HSvyS4ILtHXmpkj1mZ5yB0JGy8iya2dGLruawHbnz1/2BXz463ADn9HS+jZ+fCtyv3gaRYYjnIo3OIP8iPZpGANTH4eiPOtY+/we+OH14OurD9b/YM/p990oiktpkQ2fbm7437XaUFEa+5pcjaJgqf1eEScice7bNkVQqWdgrT9gppFRQeGjtysoCmIIinZ97XdlWaSgaN3TCo947Nxiv10nb7Uv2qm0yAqXt6+Gj34TzhsVJSg8o8TdnkbxhrDm4T/uxnnw6e+sUHLNQO4DC3b9+9fFr3uyLB4HTxwJs18OhwnrPB2Kl+pqeKgnfHRjwxx/ywq4p3Xt5rZfONaGk9/f0WaNrgt/6gSv+dPVOZgk/YZev+aGH8PmbbftaCJUUPjokpNJRmootkbhCoqiPMhsY3/ve1pyDlvXFOX2tOP1qGY8Aw/vA9+/HN3QPnVU2IbpRlYVrw83zH5n9ubF9juUBkWr7W+vVjLzeZvwsD4oWGa/N/wYzo/lFWwuO7fCson1c849jWSDIOrCok/glfObNlxzh9MBmvNawxx/41z7PfXx5MpXV8NrF4bDyZd8VvdzLxkX+xxeYt3/TfPDv9f/ELYY7FBB0awIhYTe7a1DO5CR18J+Z1nHddtecPV4OOf58PZhl8Axd8P1M6P3dRP6uSO3veGznfePLg/w/v+F5+o+yjMgz218XW3DVIWP6xcs+Yvsd1absDrrlvE+sHUNuy1cA08eZQWhq7GY6rDgKg8QFK9eCC+dGTmp07rZVjDuaRgDa2clV3buO7ZnuinBSP668tFvYPGnVpA3FW6oaEar+OXqSgsnp1rRmvjlXPy9fa+lIG9mZFRh4P4GZr+S5DkSZJLesjz8u7QQMlvb36pRND/at0zn8wWbePLLZdEbM1vDuc9Dq852uccISPHkVjzjMTjyt5DTJXpf19zjNuiuo+qEB2H/82JXyG3Us9qG1z3YzZqUvELh+5cijw/2Id7sCIoCz/W4+3ltnyUJoq9isWQcrJsFn/8x7CdZMy3cIAQNuNvg9PpcM1lZMXz3OHzyu7rVwcvYW8N+nu35du7zpmTWC1YLXJxET3Wx0yNdN7th6tLrUPsdq+fbGBSvt9+ZOXXbf9671o7vMvsVeKBbWEN3G+VN8yPfhVj4fXppLezzO+c1ePpoawqOx+op8N4vo9dHdMLccyTQ5LwBMFUV4Xu07Iton2cjooIigP27Wyn+yISlVPnDZJMlPTtyud0+thdXWRZ+eN3efXZuZC8mAglrDV5BATZRoNd/4aqt3nDbqopwQ5k3zRn/kR02bxTlhcsWrk7q0qJw7airvg1rFOtnwxon/rsiQDtzX2bXx/JgdxvFUV4c2yS3bZ0d6ZqIaU+EX+6/9oPHRkRuL8qD969PPCK+vih0eravnAuzXopfNlZAQn2R1c5+FyyPXF+8wZogg5jyH/jiT/VXh3gaxY4tiUfyv3kFvHVVePmT26w51zV1eu/dl0nMLuk31aZl2eO/+3O7vHoK/Ocw2wEJOl5pjGzO3k5crLFN1dWR7/DWlZDd0b5T1RWQ4QiKGc/aQJcmQgVFADceM4CzD+pOcVkl+9wxlgufnEpF1S6Oht7vLBvp8Oc+0WpkdvvYgiI1I/zAZfh6YNUVwVEa3uPPeCbyxRt4kjWZucf0Coe6Cgo3VrxyZ7ih8xJkenId+lUBYX+xXry/7wuPHVL7+rk9WJcPb7TaV33mwhp7C/x9cPC2lh3Dv2c8E/84Naa7Bho06Tairp/A5fVLbFBDYYC5Ztzt8OVfopNf1hX3/0jLjt72lz7xG8SgBte15rg9eK+g8Odh81KUZ4WgXyinZlozrZdN82wH5Iv7bAcjQluI8V95OyKxTE+f3GId6O7xtq6Etr1t+H1Vhf1uBqigCCArPYXrj+7HPrn2QZ6yvIAlG+sQ6nmXZ9bWQ35mvytKYP4HkeXa94stKFIy4Ic37O92fcMz7oF9kBKF8316W7ghB6u9pGbacRdfPmx7Sy6xXqoZz8G892Kfw9VYQmkE2mA3L4JnT4K8GR7zQBxB4Q8FXDO9nh287syDzmJ1de2cu4WrbePgtfNPexK2xbBlp7UI/3YDIGIRcsyYDaVRuKa+Ep+gcLXQ6gr47+HhZ87LrJfgidHR+yZDVSX8bZA1GdWMDYihOWyab53u5TusAPaaDgM7M84fWVVhBZ23YxTyjIguL7F1cIMonjneCkH/9aRl2QZ6n2OsJcDPfR3gs9+Hl/1C3RUc3v8wlv/PHU/hCpXiDZDT1Zqzqyoa7jmoJSooYtCnQzYTfjuGL377EwB+XFtY+4N4fRc5XeD3+VatLPDMUtf7SGjTM7Ix8VJWZBt1sNqFd3R4dWVyoadeU1RmTlgofXG/jfDoMdI5Vwxh+NGN8Obl9ndpEfx7mHXy1dTRHX1aGR2a67L6W3j6GJjomDDclylI0HkF1taV8Myx8MmtMS4uDomc865seOJIeKhX/LLb1tkJqJZ8Dv/c384v8vgR0X6HIIHjvSf+sTZ+akxP9ZzPK2+GDUl1TU5+jcKdrre6ykYNvePJMuBqOePvsoM5f3ij9lFTpYVWk/j45rC50f+8LZ8U/v3qBdYUNu1J+Mox9/z4lvX1+HHrV1YM/xxiRyy7eANGNi+xdRjvzI3mCnZ/VJ6khHvzoVQC8Q6Y8zfmrkbs1TRiaYhu3d16VpWFBVVVeWxB0chRayooEtC7fTYtM1L53ds/ct9H8xPv4KdF+/Dv1HQ48cHw8g2z4UInRNA/vWoQqRmRAqWqIrnEg14ndWbryPEeW5ZDz0MBsXbeDT/Gzi1Tus06Wbcsh3F3hNfXmJ5Kg0eQelkzzX67L04ijcL9HW+0uRfvyxnkG/Hz+T22YSyLM2vgiq+s2euF06zDHazgA9jiC3gICgX2miDiOdYrSmGHo4VWV9reZTLO2GRYMt42YG69Swp8BRxBEdTx8Gt/426Hr/9eu/N75493j+M1ZeUvhhfP8O/k1Mkp//bV4echlGoDFcp3hIWc+xx6B3q6+97TGsbeHD6u9zn1/2emyhEUqbEFRbxoQbeOERqF81yKT+OuERQV4e+UNKsJVVfGNmvV9zTLCaiVoBCRtiJyQENVpjkSCgl/OmsIB/dqyzNfr+Cs/3zDV0vyefbrFckd4LrpkaGyg88M/27XJ9zDbNMz8bH8o8GryqJ75MOvIi4ZOZE5qqorIb2l/ZSX2F7ysyfAgg+j9135FcxzBiN5ExmWegRFshlza64hgaCoGZzoeWHipT/wNsqxNCQvX/8j/vaVX8MLntkJoxytvhc/SBB4/6MdBfZaHj/SDvLy8uoFNqIH7P/yt4H2/6gP3Cg9l4oS65NwcRswvz8pVkM1Lc5/AFC8Ed68Mjwzo9dG7z4j3gY9yOzpNtKxeuN/7QfPHh9ubAOFtOfe5013jkekAPb7Xqor7XOZkh7sc6s5iPvTV7+yAKe6W8bfwLt1d5+RqnJrbk5kempkk1RCQSEik0QkR0TaAbOAp0Sklt2J3ZszhnbjuSutE/X71YVc+sw07v1oPuWVSUj17PbQoV94OSUVLn4bLnozslz7AFuon9TMSEFRui26B+gVREFk5ljNxktaphVY3pfn9Uusyeqe1uF1b14ZjoxxzVlLxkcKDX8P2J//yk+gM74w/NvtuW30+AM+/q0dfRtE4arw70RjBZZ+HrlcvDGyoa+uhudPiSzjF4T+hIclm6K1KlcYHnCBNaWVFtnpcf1a0nLPAES3IfDnC6srQQ1LRJSTIyj8WlgsjSaWidFlzVTbqXjzCitcaxJESvh3eUm4Nx5kSnHvbZCwqplm+EeP6cnXMUjNsv/F5qWR6zf+CA/3DS/7BYxrQo1nenLP//kfbWfCi/t/R2gUlcHX4uZBqzE9OedNZHpq5AzRyWgUrY0x24CfAi8aY0YCxzZstZofOZlpLP3TSbTKDD8489bFMVfEo/+xMOD46PU/fQpG3xJedgcOufgTEZYWRTe0/hBaP5mtox+y1EyrUbjjMFwWfxq57G0kK0qszfflc3wOcd89aZnrq4CBjR4TXlV5bNUdYvtggiKjtqyIDIV95dzoMkBNo/idL/Pm3wbAf0bFP7dfo/CbEp4YDf/yKd1uw9iine1Fj3X+43imsSBNa1dI1AOt0Sg8ddq0AKY9FVw+Uf3c57K6Av7UOdwYS8izr/E00gGCwu3NJ4wAc+vuExTpLez4nkcPjr97lEZRbf+zUBzTk1vfr/8e/d48c5wVHt73rDqRRuHck8oyn+lpN9EogFQR6QKcB3xUHycVkXYiMl5Eljjfga2biFSJyGzn80FQmcYkNSXEc1ccwk+HdQPgrP98y4rN9RQyCHDAeXD07+GaifB/38FBl0ZuF4n0UbiCwhtJk0hQZLSOHj+Q1sK+VH6CzE9egsws/nVutl0XY+C/h4aXq8qje+muGaJipw1fDMK1R4+/22bhhUhtIi5xHIHeyKWgeH5/Y+S+6N5Qz+L1kSacqnL74vd3OgfrvneOFWAqCTr3n7pGb1/5DUx+OPb+frwNS9eDbNqZ9v3D69zrqMn4G7JCc+L9wcdLlNDPL2RrUlBI5PPnni9Qo4g3psRTvqbuvv8mLTs6NDqIuKanOBpFvHvw8c0+05NrQvXtU+OjKHeO6Zw3Jc3pRMUaf9H8BMW9wDhgqTFmuoj0BZYk2CcRtwETjDH9gQnOchA7jTFDnU8Tz+tpGd67HX85J9xjfG16HccexKPbQdBxULjxad0TfrPQ/vY6vUsLraDwOsz98d9+MnOizQZpmcEmoPyF8Y/l7VW6L1SZX1B0jFzeOC9yuTLAz+JqFF/cD4s+Dj636/T95l8wyQkQSNR4rZkeHhGeDEE2b3/j7r7ofkH7gGdkvvvy73MUdBoS9kvFG1jmbWiDNI/nT47diAfhbZxDqc6ALu/98vfKE6SaiKVRlBXDvw+ygy+9uP+XSOT/XXNtcTSKRBFgNc7sAI0iGYJMT1WViX0U8SIOTVWwM9tdV+Pc90Q9VVfZ49YIijg+ikZ2ZsfSq2owxrwJvOlZXg6cvYvnPQMY4/x+AZgE1EPuhsYhNSXE93cdx+3v/MgTk5eTFgpxwzH9SU+t5yAy18zUqnM4JYjX9FSw1H66DLXL/Y+3JqR4pGdHv+SpWcG26EQTKnkdkC062Ima/CahbJ+g8EcXVZVH16d4A7xxOcx/L/a5g2L5E9nNn3Espv2Oi1/OJVCj8DXabiMVc2Q9jqBw4vlT0sL3Op7pKdl5xqurk0tI6W1wXJOKd12Q6Snu8WLc67wZNhLMHw1WIyhCkf+3a3KJp1GYKjtI0otJQqPwZ0eIhf9emyrnP4sX9VQd3/zmDxV3zWd+DSHkMT25x0tNj216cv/v5qZRiMhfHGd2mohMEJF8EblkF8/byRjj6oQbgE4xymWKyAwRmSoiZ+7iOeuVttnp3HbSINq2SOPRiUu5+/1a9FSTxY117zkyvC6oh5O/EH49B8570b7wR9xkTVheug6z31ntAkxPWcERQonGaHgFiavVRPkoYv21DlXl0edZ9318IQHR2TQnP2zHOQRR1xn2gjSKihIie9uuoAholNzGrKo8HJKckh5ujOOZnpJtsGMNWvPjbVhSUu0nUKNwfQkJNIpYxLrXrqBwTU81tnk3OWVQD9kz0nrmc8Hb3GNCdGcn1tgkP3UxPUH8wa7VVcE+ipimp7KwoIgwPfnK1zjKm58z+3jHmX0qsBLoB9wSdw9ARD4XkbkBn4hgaWOMIbbRuJcxZjhwEfBPEYkZGiQi1zpCZUZ+fn4Sl7Xr9O6Qzfd3H8+1o/vy2vQ1TFxkG85Hv1jCzW8mGfcfDzej7AEXxC9XWWqH/bu92mPvgf6+6ROPuhP+UGjVcX9PKC2rbpMMlXjucwsnh1CUj8KnUfipLIuOEoqXdsHFP2Bs4v0wNYY/o65pEGL16t2MnhDuKfojySDcCHpTMaSkhxvjIEHkkuz4iXjCxovf9BRKjVwnPtNTXYVrIkHhahRunqeahi9AQ3Hrl6hR9NfdJWmNwncPqyrt/5pQUMTpSFX7TE9RGoVrenKjnio8giLNY3ryXXtQRFUjkJQz2/k+BXjTGJNUqI8x5lhjzJCAz/vARsdBjvMdaOMwxqx1vpdjzVPD4pzvSWPMcGPM8Nxcf6RNw/LrY/rTpXUmVz43nVenreavny3mrZl5lFXuotTf76dwex50HlL7ff09wrQW4XXZvvsTlNsmGbyCwnWiR2kUCQRFkDM7qXPXIo1EaqZNQ5Is7gjhWA25V1C4fpGgxswVNG4kC9hvt0GLKyh8AjfWSNx4x/ASZXpKi6FRlEQu15ZYDZirAQr2/3bzltUI04D93EYxUdST3xHvkrRG4buHrpYWSo3joyC+6SmWj8J/Ld5xFF6NIhTDR+EKz4bKBRaDZATFRyKyEDgYmCAiucCuJt75AHByQnA58L6/gDO4L8P53QE4HKjD0OiGJzsjlTd+fih9OmRz+zvh2P2F63dxVK1IcIbNI26Cgy4PL//y2+gy3t5ip/3tVK0uZ/4XTvt3eDmtBVzpC4X1kxIwe1+Q6am8GDofYCNrIHGvrqq8btNPxkocGET59vizqXU5MHLZHSGcjEbh9oSDGsiaRrA8UqNwX/J42oBfo4jVKNVJUKTV3keR0Tp4vZ9YvWy/6cn1pVXG0ShqZmxM0pntr3vSzmzffm5esV02PcVxZruCOJ7pqTpIUDRT05Mx5jbgMGC4MaYCKME6o3eFh4DjRGQJdkzGQwAiMlxE3CGf+wIzRGQOMBF4yBjTLAUFQI92LXjuikNolRF+sL5aks/GbQ0wW9mx98BIJ59NWjZ02i+6jPsiDr0Efvk1ZHvGZLRoBwd7BE1aJuQOgCHnRB9nxLVw0zybqMxPhOnJE3nlfblSM+F3ccJWK+soKMq2JZ8mPJGaHisAIBlBURVHULj7R5iePEnq4jXyfhNerAY4WdNThKBIie2jqBnv4NMo6uoYdvFGPXlNT17zXNSxXP9FkhpFlI8iyTrH0ih2RVD4NQrjFxS+qKfK8vA9iDA9+QWFOz7F6/9o+AioZJzZacAlwOsi8hZwNeBPFFMrjDEFxphjjDH9HRPVFmf9DGPMz5zf3xpj9jfGHOh8J8jP3PT07pDNuJtGM+uu4xgzMJe/fraYkQ9MYOLCOk4IFA/XJ5AWI0dU7gC49F045a+Jj+Wm9HAf2rOeCDeeGa2gdffgqJ4gjQKcOS48L4LfrJWWDTc7o2Xnvh3pkE3WNl66LfnIoETEFBQxetfJahQ1gsKnUbjEc1jv8L1isRqlZPJZQbCPorIUPrrJmrVcuRDLR5ESp8GMqGcCjUJCtlGs8VG4DV+QRubcv4S95wAfhaQE+42C8Ats93+LF/UE8U2mMZ3ZMaKevNF/KRnhNONRPoqA5y1RtF89kMxb+V+s2ek/zucgZ50SQNc2WbTLTufPZx/AsJ5tAHhxykq2ldbzn5ndEQ6+Ei55O3aZfY6OH7bpkuYTFMaEe75uTzIoaWFJDEFRssljUw+wdadlhUds5y+InP7UP+dGLMoC0pckjc/eH9Rbrq6KLYi8dXRt60E29limJ5d4gs4vAGJda9JhtH5B4fy/M551euIJfBTedN3xiKlReAfceTUKT+qKqGO5pqckR5V7I/dCqcHm0iD8ArsiWY1iV5zZDt4Bd5VeH0VqAtNTQHqQBiSZbsIhxhivEfcLxxykxKFTTibv/t/h3PXeXF6auoph947nysN6M6BTK0b0aUfvDkmqxbEIheC0f9ZLXaMFRbWjZWwNq+9umZMettO/PvmTyFQbfqe1GxYb1KvzOxnnvhX+nZmTXNTTrmgU/v2CBEX59iSd2XGiULwahStovaanyp3Jj4OINR9H0uMevA2UiWwAy4qpEZ41I7P9giKgqaiuinb2xmo83fEzNaYn10fhSYbnx9Uo6pLCI5QamSU5FikZ0f9zpddHEceZHS9TcsIBd465KNCZ7TU9+QRokIO/EWZqTEZQVInIPsaYZQDOyOzG9aTsxvz+1H0Z0acdn83fyNNOxtmRfdrx+s8PTbBnI+K+DF5B0bIjFK8LOwTdhq7vGGvWSs2MbBRatLO+iD878zqc9bgdC9EpIGIrlrkMktcoKkrqPuOa3/4fFDBQXlK/zmx3H3+obsWOxHNUeI/lJ1lndsRsaybSlFS2LawRxQqTTgnQKCp2Rtc9kfA21YDxRD15ckJFHT9ORJSXoKSAodTkwqJTM6N9FO41xM31RALTk2+wnCsY/L6KmM5s1/QUI+opyKzVgCRjeroFmOhkkZ0MfAH8tmGrteeQkZrCaQd25d8XDOXFq0Zw/OBOfLdiC18viQzvXLhhG8PvH8/STfU0/0BdGOCMvehyYFhDcHv/rkbhhsH6Z2pLy7a+iAtehZ9NsIJj+FXBA7fimcOCGu1YlNTR9+OPmAoKo0xWUKz61smCmqwzO0BQJENMH0UdBtz5NYrSbeFGym00o2zpAT3roDolMgducSZO8vso4pnuEpoYXW3I8+6EUpLTKFLTA6KePKaneD6zWg24q/R9u/NTeOajqHFmO6anqrLogYiVQc7shjc9JRP1NAHoD9wA/AoYaIyZGH8vxY+IMHpALv+6YBj9O7bkkme+Y/j94/nruEUU7ajg8UnL2Ly9nLdmxphOsyH45RQ42xMjMPh0uH0tdDkgnHrDjSRxNQpXUHijqCCseQw6GboPj3/eePHtyWoUAFMfT1ymW0Bd/IIiyPRUVmwFQCgt+hiZnjqunQnv/jy+oPCPo4go4zTMiWYsi2osa5tyw9u7NZE+h9KicI/ePZ6/txzkowjqUXsFV7z/0u+jiBcemygqLujeh1KDtSAvwy4J1ii8pqd4I9QTRT15tTi/M9svMLymp1RPmnE/L5xqE2E2F2e2iPzU/WAH2/VzPqc465Q6kJWewss/G8nFI3vSuXUmj05cymEPTeC92esAWLTBmkWqqxthqsNOg2F/X0isa0ro4aTrdgVDWpaNpXdNFl7nNSQ/uMk9FsDPA1JuBGkUsZySiz+x3617wuCAiO1j7raNgR9/vinvOVs7E0gtn2TTp7frC6N+GVneq1EArJ0VP2onljMbbCNVVQGrvone38sLp8J714WXxRfOmogojcKjIXhDjWv8Ar6ebFCjG9TT966LJyjSspwIqHjObKcuibIGxBIU8WaN7DEKznjM/h/+NCjuPU0U6ZVIgHn/G78zu+bbndmunKhxFLH45l+RPoom1ihOi/M5tcFrtgfTMSeTP521P09dNpwh3XIoKbd/+vBebflyyWZem7aavneMZcqyXYpC3jUOugyu+DjcAA+/Ek58ILzdr1HURlC44bidPfM2DHHyTPp7952GhLfF4rwXYswQKMnZqb3nPPpO+z3hjzaHVsuO0WaXoGsN1ChipPCIKLMDxt0ZPUFSELP/5xyvMtyQF66BD34VNukEUbotcrs3qg3seBjvZFFBBNnqvQn95r1rl71O93gmxpT0SD9XPB9PonQmQWarUKoV8rFwgyyC/kvvgLu4500gKLwCLkqTiKNRJIq2AhutVlOPJox6MsZc2eBn38vp0jqLj351JKsLdrC+aCf7ds3hmL9N5jZndPdbM/MY1bcdM1dt5cAebUhLqefstPEQgd5HhJe7HWw/Lm4akM4HWKGSbMw6hMt61fqgsQy//sFqNGlZtmH84bXg46VlBffuJJTY/ACRgsKvKQWlIPHbvo2Jr1FUlgYPuANr6lk2IXEdXSY/DH1/El6e84r93rwUrvokuvzrlwTMK+LzUXwaK8u/h0BB4TSoSz6zM9mNvjWyd16QYDYCr3klnkaxc0v0Ni+BGkVKODtAEK62kRmg9XgH3MUjkUbhda77ndjut1eTi4h6SnDu+Z5kFs3BR6E0PD3bt2Bk3/bkZKZx9RF9ata/PSuPPreP5ZzHpyQ/R3dj4Tau/Y+DEdfUbt+g6T2DBEXbXvZFTkmL1GCCGq0gp66EEvfM/Od2BzK6+NOkQ7QpLNaL6vZMy7eHzVtRGsVO2B4nieUxd0cuT7w/PKNa7yPD62M1pkGTT/l9FMkQJHDdhs0dI7FluR1s2WGAXe44OPbxdhb6NIo4PopEBO0bSo0fXef+D/HMY4nuUSJB4fUf1Uz56hcYTt2LN8Y2PQWZVb00kwF3SiNyzZF9GX/TaP578UH09Yy1WLihCaOhgnAbB/90rbG44mO47AOb/vy4+6K3J0oR4Y0+8c7i1/tIO1Ob+9L28fS2pQ6mJ79GkZJG1OCz1HRo75kHPdaL6s5A6B03ECUoSqJ9Jl2GhrOKdh8RfdziDfa7g2eGuiCndqywSVMdf3yAnxYd4msUrma4+FNY8x1sXgy3roBL37Prg8w7O7fa+1gZR6NIVlAENdhufW+PERziahTxouxS0uIHGSQ0PXne2UkPWCHqnxLVXS7Ki4x68gqKvmPgtjgTpKlGsfeREhL6d2rFSft34Yubx/DKNSNJTwnx7vdrufv9ubz7fR4mUYRMY9DPmQTIawaJR+8jbNnRt0CfI6O3JxpLMOwSQKx/4/RHw+svfNU6Hd2JnY64EUY6zucg01O7gEz13sbbLyiCIk9SM+FXM8PLQQ1yeitYOyNsfnB7rlGmJ58z+qSH4cqx4bQnQQ3ZNmcqF68Nvnh9dM6fkjiaSjImOYBj/gC/+Dq+j8L1Ibg2+T6jrWbWqhPcUwSH3xi9b5/RiX0UFaXBGl1UuZ1w6PX2XO5/7zqiYz1Xrv/Eb3rydkgS3aN4A+4gWni/f11s01PRmrDzOzUjch6XUGp8zcd9/lZNabC8Twn18hgRTkXAj8aYBkhipHg5bJ8O3HbSIO79aD4vTlnFi1NWsbO8mk45GWzYVsrFI3s1TcX2PRXu3JBcipBk8PY6f7MwOiwxdyDcUxi9n2s2OupOa5PuexQscjLhBpmeuh8SOfvaFR9HNgj+RHL9j4ue1Mk/hsTfG77kbVj5NXz7iE1R4q2nX6P4wjedae5Aq+FktbX5kfwRVgAbnQzFrbuH11VXwtd/h9E3w7Mn2el0h8QJTkzGJAfQ7xgrhANNT06P2u8Iv+DVyGX/M3LsPXZa2JSM+D6Kyp22wUw0XsZUha/HFRCJrs+9d/4GOCU9LLwSPduJItX8z82qKfa9gUiBEXJSz6+daX1/6dnQxvNeh1Ljh+lWVcDicfDKeXDyX2tvCk6CZDSKq4GngYudz1PYaUu/EZFL671GShRXHdGHFQ+ezPx7T2B4r7bc8e6PXP3CDO58dy7byypZuqm4abSM+hISEPli53Sx07/GY/9zrXnGfYFSM2C/M+2yq7F0PSi6gfOnFO99RKQtOhQK9ypPf9RqTv5j+JMc+sMrU9Kt+ai6MhzNFMtHsX1D5LJbzhuWHETbPjaXl0t2R/jiPhh7C6z+FqY8ah3cQZjq5H0UrgCPZXqa9RJs8GT0kVC0GdF/Da4wTs2I76OoKI0870/iON3dcu51JRIUbkPs1yi8/0+i7LMbfohed8NsOMeJSPJrFDXzYmOfDWPsdbdz/JKLx0HuIPu7rU9QxKOyNDy//aaGSbCd7MRF+xpjzjbGnA0Mxg6FHMluNM/17o6I0CI9lfvPGsLZB4V7kr/830yO/fuXvPDtyqarXL2SpMA7+2n4QwwH7r6nWRt5z5F2Ctihl9gxG4fdAAd6ZgusMVP4Gk23sXHNUP1PgOFXh7cnGvEbSoueHKrGR5GggXY1CFdr8QsW13Q24MRIbcONSJv2ZHjdu9fGqaPPRzHoVDuVrh/Xlh8kWNbPgQ+uj3SYZ7RKnKLcvX8t2lvz2OalkXnDXKrKIk1B8TomNRqFU09J4INxG2K/RuFtlJNJrZ7TLXK5VWc72yRYH4V/ZPcyZ6zy1pVW44TwgM6qcui4r/3t/W8T+ZPKtkGh48NINu18LUlG/+xhjNnoWd7krNsiIg3vblciGNQ5h7+ddyB/OecALnpqKl85qUDu+XA+01du5ce1RXRuncnr145C6jrvcWOS1sLaZt1xEB0Gxi+fLG70UkYrOPMx+9vVJo67z/pL3GW3cXEbw5R0ZxY2t3FPhePvhxlJZrpPSYtugGp8FAmc636NwttwnfQXyJtuTWftHYFx41xrenEbnWTwj6MAq1n1DvAduRpFkOP2m39Fr0sP8Kn4G3hX+LTtDSu/gkcPjtqlBq+fJe64DJ9GkYgajcJn2qv5fyQ5jTkjB/A4zL0D/cpLnFQcHl+GN3Bh/F32u4MnMKLXYeHf6a2ChY2f0m2wyTFxFsTQIneRZATFJBH5CHjTWT7HWZcNFDZIrZSEpISEl64eyesz1rB2604en7yMj3+0Ts7VW3awYH0xg7vmUFpRxe/e/oEtJeX8/byh5LZKMvVyY/HrH2x0UId+cNW44Cif+ubwGyKX3QiUNj3sd02KdY8jNFnnL9jGISrFSZIahStQXPOWV7DkdA3P6+Da2GvqXIv/1VRHmzMycqJDgyEcYhrU4w8iqBfuHxDmahRteyceWe7tIccbae33UcTTTFMywqZNv0Bv1cmaA9OzHc0ogYbrN0NKSrieZdujBUWsup/1BGxeAoPPDK+/8mP49HY7Q2U8vBpFwRJnfpH67SQmY3q6DngOGOp8XgCuM8aUGGOOqtfaKLUiPTXEpaN68bsTB3LNkX24cESPmm0PfrKApZu28/7stbw/ex1fLdnMy9/FmWmuqWiZG+5R9RyVXMrt+qZ1Txjxc7jYSXfuNmTeiKNknb9ge4B+h7d7LLfH7W0QvLgNc8tOzgQ2aeFzp2VBT6fHmevTvGoz4BHCPe+cbnDGf8ImuV9Osea6muM6veodMcx8fhNPkK/B1dzcsl6NIhHlnrky4pqefFqh12d3vC9g4K5NYXOO10dx7eTwGJBkMw34BWMoFL6+kk3xQ1ddTSGUZu//MXdFNvBdDrQRcF6NI4jSIhsyfeBF8KtZydW7liSTFNAAX2Ozxk4AvjTNIj5TcRER7jxlMPedMYQ7Th7EpaN6MWVZAac/+jV/+GAe3dpksU9uNnPX2jxSVdWG1QUNY8vcLQmF4OS/hM05rqDwCoegHtrRd1nbvl8oYKIFnmvG6n6InZ/cdXjGYsQ1cOUntkFz65GaBUf+xmph/vQUR9wEWQEaQRD9jws3lCnpMOzi8HKnwdbH4+JeR6wBfa7J0D23P9IHbFr6PxSG84e5Qs1tlFv3iCx/2QeRy65WlYyPwi3j1VQO+xX8dlHwft571nVo+DjJzrcdlHTSW8942oSrySbbCbnmi+D1W1daAd11qNVkG8DknMxUqOcB07Amp/OA70TknPh7KU1BakqIa0fvw31nDmHyrUfRsVUGpRXVnD60K/t3a828dUVUVxseHLuA0Q9PZM0WFRaBXPAqHHxFjPxRHkbfDBe8bENIvQT1o1zTUygEvQ61DfPlH1kndJBJJbM1dHds9zWNYKbdzxsR4y2faCIrCVkn/yE/8zh9A5oAfwQRxDY9uSYw1wkbK5utSLh37V5vh342yOD6GTDw5HBZ//1wBXdqHEHR2THPDDnbzv3uDxH1DtL04h8342o9sabH9fOTW+EX38B102ySQW99kyWZibrAPivDr4pcl97SDnCExJGCu0Ayev6d2FnuLjfGXAaMAO5qsBop9UK3Nlm89cvDePaK4dxy/EAO7tWW9UWl9L1jbM0ESrNWJ2l33tvoNBhO+1dwtIk/ygWi7dxu4+uWPfae4GP1OdL2En+/MXqbF3ffROaQoDEXaS3gnOfsb1Nt/RAiYWEQT1B4/SmuOapl5+CyblhnvDm8XUHh9bt0OdAKwAs9Yy/8Da1bj6CUHF0OhF6Hh53AWW3gnGesoPcSq/H2+xiSvdfe8p2HWFOgm6k4li/ldE/AwchfhH/7Nap4+DshWe0g3xUUXZI/Ti1JRucJ+QbWFaAjuncLOrTM4OhBdoTnBSN6MnXFFj7+YX3N9ukrt3DG0ICGTwnm519CTvfo9a6de8S1djyD27v9tTO+oDaO8CBqTE9xnLkQLCgQ2O8seOvKYId3oKDw2fvBhs5uXWXThlRXwoPOfXAbf1dQxMMtm8jU4o8Mc+sRpFFcMdbel7qaW/wCPFnT06BTgzsNQcd0OegyaqKp1jvPxshfwgHnJ13dqPTvWa2hyHFke0dz1zPJCIpPRWQc4Ir884GxDVYjpUFISwnx2EUH8eezKxnyh3EA/G/qalpnpXHTsQNIbczMtLsr/sF6Lq6/oH1/GOkZu1AbAXHjj0TllHKpcQInMGlE+Urc/QVO+VvYEQ7RczZ7qfFfeOqf2dpOaOXHHUDWqhOMuT1yEGCssokERayQ1SAfRUpa4nkjakPIZ3oafCZMfzq63AUv1+34BzljlN1R3e361C6Aw30GRt9ic3AtdrIQ5HSLHKlfzyTjzL4FeBI4wPk8aYzZpYF2InKuiMwTkWoRiTkdmoicKCKLRGSpiCSRC1lJRMuMVBbceyITbx5DekqIxyYuo9+dn3Dlc9N47/u1FO2s29CYu96by7F/n1zPtd1NGHYZ/PTpaPtxbWjTMxzq6qf/cfY7UVy/f5AfUBPeecjPrEmtZnUcQeEKiGSykrplstrCmNvCDuvAskkKivRsm7dp9C02bNR1fgftV5totGRwzYiuiafPkbYu9Y2rXcXyncTiqDtg1P/ZezPqF2FtdtT/1S7RYy1J6i4bY94G3q7H884Ffgo8EauAiKQAjwHHAXnAdBH5wBjTMGPU9yKy0lPo0yGbuX88gV/8bybz121j2ootTFyUT1qKcNKQLvzj/KGkhJJX51+aakNvS8oqyc6o55e3uRMKwQHnNtzxT/0nHPnbxI1KRksb3VNeYmfne/+6yDlFvLjRPn1GR29z9/FPGesls40VCu6AuETpLsDO7bF5Ufz03xBu8I7+vf12B/YFphOvReN4+YeJRy73GQ1f/RXW+cJMJcUObEzLju+H8fPLKfDfQ6PXH32XNQUmSiHuJ6stnPhgePnYP8JBl8fX5OqBmG+0iBQTPNpEsFGztZjcOBJjzALnHPGKjQCWGmOWO2VfA84AVFDUE+mpIZ694hAASiuqmLFyK69NX80Hc9axo7yK647ah0Gdc8hMCyU9ynvOmkIO65dk6nElOVLTw6G7iXAjX9rvY30lQdlyAVp3g+tnBkdQtetrHfDxBNNtzpic1y6Gdd8nzv4L1qm+5LPE4yf8WkL/420OI9e0ltXWahrz3k18Ti9BQhHsiPSVX9nfPUeF13lxp0y9flpyqetvmmfNV1ltbESU3znesiOc8tdaVT+Qdn3CuaIakHgz3MVJ1N4odAPWeJbzsPmlAhGRa4FrAXr2TBDWqESRmZbCEf07cHi/9gzt0Ya/fLqIzxfYaJxbThjIFYf1Zkd5VeDI7kMfDM/QNievSAVFcyGWT8Ul3kCuI25K7hxnPGp7xf4BgEG0zLVjNhLhFxTH3G3Neq27wcVvQ8dB1h4/4ITk6piIS98LTyiUmgE3LwnIKptmBYWb2TcRXn9BMvemmdNgHkwR+VxE5gZ8aqlrJYcx5kljzHBjzPDc3CBbrZIMIsLPjuzLxFvGcN5w+7A/PG4Rw+//nGP/PpmN20opr6xmU7HN+plfXMb6ovAEM6u31EItV3Z/strCAefVz7HcGfH8o729Y0f6H1v/TtuU1MhAgZYdo81j5zwLPUbGnxdiD6bBjMnGmGN38RBrAa93rzsR2beUhqRbmyz+cs6B3HLCIB79YgnLN5fw1ZLNXPPiDKqNYe7abdx3xn58Nj9yDMDKzTqIT6kjl31g59poijQuieh/XDioYC+kOXsdpwP9RaQPVkBcAFzUtFXa+8htlcEfzxgCwPj5G7nmxRk12+56f15U+dU62lupKy1zoWXDOmWVutEkoltEzhKRPOBQ4GNnnAYi0lVExgIYYyqB64FxwALgDWNMdMukNBrHDe7EK9eM5PFLDubUA6JHgXbOyWRt4U5mrtIR34qyJyF7Yn6/4cOHmxkzZiQuqOwSW0rK+dWrs9hSUsGC9dv4+ei+fPTDelJThM9/8xNSPeG1u8XcGIqyFyMiM40xgePaVFAou8w7s/L4zRtzePGqEVRUVXP1C/be52Sm0iozjbQU4YvfjiFUi3EZiqI0LvEERTP0Gim7G2cN68b4m0YzekAux+zbiTtOtnl/tpVWsrZwJysLdjDwrk94eNxCSiuqavYr2lHBE5OXRaxTFKX50Zyd2cpugojQv1N42M21o/fh8H4dyG2ZwX8mLeP5b1dSUWV4bOIyvlqymQfO2p/BXXI4+d9fsbZwJ73aZ3PikMispN8tL+DLJfncckISyeYURWlQVFAoDcJ+XW1it3tO34+jBnVkn9xsnv5qBc9/u5JTH/maAZ1asrZwJwATFmxkzMBcMtPC8fPnPzkVgOuP6k9WesPlsFEUJTFqelIanJ8MyKV72xbcdepgJt48hkN6t2VlwQ6uPLw3AG/OzOOAP37GCf/4ko9+WEdZZdgUpeG2itL0qEahNBopIaFPh2xeuWYUVdWGzLQUnvtmJQDlldUs2ljM9a98z8+OCOeuWVVQwsDOTZ1NRlH2bjTqSWlSvltewMqCEqYsK6BVZhqfzd/Axm1lZKaFKK2wqbC/uvUoerRLcsYxRVHqhEY9Kc2WkX3bc/4hPfnnBcO478whnH2QzeNz5ynhuRPu+WAe/520jOpq26lZubmETdvC+aUWbyzmrZl5jVtxRdmLUNOT0qy46bgBHNG/A4f2bc9RA3O59JlpTFi4iQkLN7GhaCfnHNyD0x79GoBj9+3IIxcexNUvTGfNlp2M6N2Onu1V81CU+kZNT0qz5se8ImbnFTJhwUYmL86Pmlv+hqP78dy3KykurSQjNcSE3/6E7m1VWChKbdGR2coeQX5xGZ8v2MjMVVuZsXIL20oryUpLYcO2Ujq2ymB9USn7dc3hzKHduGZ034h9q6sNW3eU075lgnmnFWUvJZ6gUNOTstuQ2yqDC0f05MIRdmKq92ev5devzQbgobMP4MvF+Tzz9QrmrdvG8s3bufn4gTWC4fEvl/GXTxfx3R3H0CknwVSciqJEoIJC2W05Y2g3Du/XgWpj6NgqkyFdc3hpyirKq6p5ddoaZq7ayuOXHEzf3Ja8M8tOZTJ3bZEKCkWpJRr1pOzWdGiZQcdWtuFv3zKDOX84vmbb4o3bOfpvk7nu5Vks3bQdgKtfmME6Z0T49rJKFm8sBmDNlh288t3qRq69ouweqEah7FFkpadwywkDGdCpFdNWFLAsv4SvluRHlHnkiyUc0S+XP344j03FZcz74wkc/bdJVFQZThzSmXbZ6U1Ue0VpnqigUPY4rjuqH2AnWgLYWV7F+7PX0r9TKx4Yu4BXp63h1Wlrasq/MWMNFVU2qGN5/nbaZbdr/EorSjNGo56UvYqN20q589259M3NZr+uOdz61g+UVVZHlJl2xzF0VD+GspehUU+K4tApJ5OnLw+/C93btuDs/34bUebWt3/g56P3ISRwcK+2pKaoK0/Zu1FBoezVHNyrLfeesR/FpZWsLtjB6zPWMGlRPpMWWb9G++x0bjlhIGsLdzJlWQFv/PxQnalP2etQ05OieKisqmbuum18v3orr01bwyInKsrlPxcfxEtTVnHzCQM5uFfbJqqlotQ/OjJbUerI10s2c8kz3wVu2yc3m/vOHMKwHm0pKCnT1CHKbk2zExQici5wD7AvMMIYE9iqi8hKoBioAipjXYQfFRRKfbO1pJxl+dt59psVLNpQzLL8kqgyB/Vsw31nDmG/rq15YOwCxgzM5bB9OjRBbRWl9jRHQbEvUA08AdycQFAMN8Zsrs3xVVAoDUl1teHzBRu59qWZABwzqCMTFm4CoEPLdC4e2Yt/TVhCSGD5g6cAdnBfVZWhdYu0iGMZYzj/yalcNKInZw7r1rgXoigemt18FMaYBcaYRU1xbkXZVUIh4fj9OvPcFYfw4E/355krDuHA7naO8M3by/nXhCUAVBu4/pVZLNlYzE//8w2jH57IlpJySiuqqKiqZtGGYvK3lzFtxRY+/nF9U16SosSluUc9GeAzETHAE8aYJ2MVFJFrgWsBevbs2UjVU/ZmjhrUseb3i1eNZF3RTh6fvIyZq7ZStLOC4tJKPvphPR/9EBYCB903ngN7tGFI1xxe/m41j1w4DIDZawoxxiCiEVVK86PBBIWIfA50Dth0pzHm/SQPc4QxZq2IdATGi8hCY8yXQQUdIfIkWNNTnSqtKHWkdYs0WrdI418X2Ia/utrw1sw8/jt5GSs2l3DNkX146qsVAMxZU8icNYUAfOwIkfziMtYXldK1TVaj1XnNlh18s3QzF4zQjpUSnwYTFMaYY+vhGGud700i8i4wAggUFIrSnAiFhPMO6cEZw7qysaiMnu1bMGV5AXPXboso9+m8DTW/56wprBEU5ZXVfDhnHcN6tqFvbssGqePFT3/H6i07OH1oV1qkN3fjgtKUNNunQ0SygZAxptj5fTxwbxNXS1FqRUZqSs30rE9fdghTlm8mNRTiuxUFfDp3I5u3l/GTAbl8uSSfBz5ZQMvMVJ79egUdWmbw5sw8urbO5E8/3Z99O+eQnhqq14SFG5x5x4t2VqigUOLSVFFPZwGPALlAITDbGHOCiHQFnjbGnCwifYF3nV1SgVeMMX9K5vga9aTsDizL384PeYUcP7gzVz43nWkrt8Qs27FVBpuKy5h08xh6d8iul/MPuusTSiuq+fTGIxnUOadejqnsvjS7XE/GmHcJCwHv+nXAyc7v5cCBjVw1RWk09sltyT6OWem/lxzED2uL+HbpZhZuKOarJZvplJPBxm1lAGwqtt//m7qK3586uF7OL1jHedGOino5nrLnovqmojQD2rfM4KiBHTlqoI2kKtpRwQ9rC7n0mWkAiMDxgzvx3Lcrmb5yC+cd0oOLR/aKOEZFVTW/e+sHrjqiDx1bZVC0s4L+nVrFPKcbYFW0UwWFEh8VFIrSDGndIo0j++cy++7j+HDOOsYM7EhWegorNk9lTl4Rc/KK6NmuBaUV1RwzqCPvzV7L9rJK3vl+LT+uLcIASzdtZ+F9J5KZlhJ4DjcQVwWFkggVFIrSjGnTIp1LD+1ds/y/q0fy0tRVvDZ9TY224ae0soo1W+x0r+/PXsvyzSX0bp/Nkf07ROSjcsdsqKBQEqGJ9hVlN6JjTia/PX4g428azekHdq1Zf8oBXWp+u0IC4I535/LE5OXc/s6P/OJ/MyOO5QayLFhfzIL1kWG7iuJFs8cqym5MSVklItAiPZWnvlzOq9NXszwgYaHLU5cNZ8zAXJ7+agV//nRhxLYF955IKGRDepW9j2aXFLChUUGh7M3MWVPITW/M5sh+HXhhyiruOHkQfx23mPIqO+Xr6AG5fLk4P3Df/brm8PENRwJ25PbS/O01DnZlz0YFhaLshZRWVLF003aGdGvNqoISfv/eXKav3EJpRTVH9OtA7w4t+CGviB/yiiL2W/HgyYgIw+8fz+bt5Sy490Sy0lXL2NNpduMoFEVpeDLTUhjSzWa17dU+m5euHklxaQU/5hUxpHtrcjLTWLNlB7e+9QPfrSig2ukz/vHD+Vw0siebt5cDMGv1Vob2aMPKghL269q6qS5HaUJUo1AUhRtf+573Zq+Lub1Fego7yqvihtsquzfNbj4KRVGaF6cP7cqATi3p1T4cPpuTmcpRA3MB2FFeBcC8ddv4fP7GmpDa/OIy/j1hCXPXFkUfVNljUI1CUZQainZUMHVFAW/OWMNlh/Zm9IBcet/2cVS5bk6W2wN7tGbsjxvom5vNhN/8ROfT2I1RZ7aiKHVm3roidpRXce7jU+KWy8lM5cKRPbn5+IGkpaixYndDBYWiKLvMIxOWsLJgBz3aZfHhnHUsc8Zr7Nslh6rqahZv3F5T9rkrD4kKqzXGUFpRrRFUzRQVFIqi1CvV1YbSyip++b9ZXHF4b0b3z+Wn//mGOZ5Q26MG5nL1EX05on8HAP47aRl//nQhc/5wPK2z0gDYuK2UZfnbOWyfDk1yHUoYFRSKojQKQf6Mlhmp/O7Egdz1/jwAXrhqBMs2befwfh345cszWZ5fomM1mgEqKBRFaRSe/XoFawt30rt9C7q1zeKq5+17mBoSKqsj25oe7bJq8lL9+ez9ObhXW/p1bEVFVbX6OJoAFRSKojQJeVt3kBISHhi7kC6tM3nyy+Vxy48ZmMukRfl8/pvR9OsYey4Npf7RkdmKojQJblrzRy4cBkB5ZTWvfLe6Ju+Un0mLbA6qK5+fzis/G0WPdi0CyymNi+p3iqI0Gvecvh+L/3QSr14ziu5ts3jt2lGcP7xHzfYDe7QBbKr0y56dxufzN1JeWc2EBRv5bnlBE9VaUdOToihNyrbSCl6ftgaD4cxh3TjrsW9ZWxieU6NPh2xWbLahuCsfOqWpqrnHoyk8FEVptuRkpnHN6L5cO3ofOrbK5J3/O4zfHjeArq0zuerwPqwqCM+vcf0rs9i8vYynvlzOlpLyiOMYY1iysZinv1rO+7PXNvZl7NE0iUYhIg8DpwHlwDLgSmNMYUC5E4F/ASnA08aYh5I5vmoUirLnsCx/O18tzueeD+cDkJkWorTC+jh+dXQ/BnRqxfH7deLhTxfx9NcravZb+dApbNxWysgHJvDM5cM5Zt9OTVL/+mLcvA28PTOPJy8L7PTvMs3RmT0euN0YUykifwZuB37nLSAiKcBjwHFAHjBdRD4wxsxv9NoqitJk7JPbsmba1iP6dWDxxmJ6tE1jyabtPPLF0pj7Lc/fzoL1xQBc/cIM/nL2AZx3SI+IMpVV1YgIKaFwjqr1RTtp2yK92WXJnbw4n8/mb6S62hAKNW5OrSYRFMaYzzyLU4FzAoqNAJYaY5YDiMhrwBmACgpF2cvo17EVM35/LB1aZgCwtaSceeu2cXCvttz53o+8M2stqSFhaI82zFi1FYALn5rKGUO71Rzj1rd/4PShXVmxuYT22el0zMnk4qe/o2e7Fjx87oGANV8d+uAXHLZPe165ZlTjX2gcNm0rBaCssvHToDSH8NirgNcD1ncD1niW84CRsQ4iItcC1wL07NmzPuunKEozwBUSAG2z02tSgzx8zoH85rgBdG/bgsqqalZsLmHxxu1c98qsmnEbB/dqy8xVW3nm6xU8PG4RAzq15KNfHcnMVVuZk1fIPafvR3ZGKpuKywD4dlnzi7By61ZaUbXnCAoR+RzoHLDpTmPM+06ZO4FK4OVdPZ8x5kngSbA+il09nqIouwcpIakZr5GaEqJ/p1b079SKTcWDmbQonzOGduWMod047KEJPDxuEQCLN27nwD9+RmW1obLa8Nn8DQzqnMM2Z54NgJKySrIzmkNf2rLR0Sh2VlTRtpHP3WB3wRhzbLztInIFcCpwjAn2qK8FvAbF7s46RVGUhFx5eB+uPLxPzfI9p+3Hl0s2c9KQzlz27DR2VtjJmDJSQ9z0+hwA/m/MPjXlpywr4NjBtXOAv/f9WrbuKI84b31QVW1qpqYtderdmDSJuHSimW4FfmKM2RGj2HSgv4j0wQqIC4CLGqmKiqLsYZy0fxdO2r8LAKMH5PLl4nwuHtmTlpmpPDHZmqj+M2kZYKd+/XTeBl6fsYafDMjl7IO6M3ddEYf0bhf3HDe+PhuAC0f0rFdneEFJGVVOrqyde4ugAB4FMoDxzoxYU40xvxCRrtgw2JOdiKjrgXHY8NhnjTHzmqi+iqLsQTx92XAqq6tpkZ5KaUUV6wpL+XCOnTP87lMHs2hDMa/PsC7S8fM3sjy/hGe/WcH9Zw7hz58u5HcnDuKiET256OmpXDSyF6cf2DXi+FOXFzDGNx/HrrBpW1nNbzc0uDFpqqinfjHWrwNO9iyPBcY2Vr0URdk7SE8Nke6MN85MS+GRC4exs7yKamO46og+LM/fXiMoAJ79ZgUi8Pv35gL2e0i31kxdvoWpy7dw+oFdKdwRHgA4a9XW+hUUxaU1v5vC9KQjsxVFUYCnLx/Os1ccAkDf3JaMveFI9u2SU7P92SsOiVj+wwfWwNGhZToAKwvCVvSvl26u1wY9UqNQQaEoitIsGNw1h0cuHMaATi2ZdPMYjhrYkbE3HMHsu4+jd/sWzFlTCMDm7eVc/uw0znzsGwB6t2/BrNWFXPX8dMA6ot+ZlUd5Zd1NRm5oLDSNj0IFhaIoSgz6dWzJZzf9hN4dsgEQEdq0SOfla0Zx4Yie/NtJnz55sU2Pfs7B3fm/o6xl/dtlBYyfv5EzH/uG37wxh/9NXVXnerihsbAX+SgURVF2Z7q1yeLBn+4PwLAebTjjsW+469R9OfWArqSlhDhu304cfP94rnkxnHPOm9ywtmwqLqNDy3Q2by9XjUJRFGV3o0e7Fsy66zjOGta9ZgrXttnpHNC9TUS5F6as4uD7xpNfXIZ36FjhjnL+8P5c/vX5Eo7666RAH8T6op30am+1mjIVFIqiKHsGd5y8LyP6tOOVn43k96fsC0BBSTkn/PNLDrpvPH91Rom/9/1aXpiyin98vpgVm0uYt64o6lhrtuykf8eWAOws33vGUSiKouzRjOjTjjd+figAh+7Tnh7tWnDnu3PZvN06ph+duJR3ZuWRv70sYr/vVxdycC87sG/z9jLSU0MU7ayo8ZP8bfxizhnenS6tsxrtWlSjUBRFaWBEhBP268wnvz6Sl382kkcuHEbv9i1YV1RKRZXh3IO715T9cslmAKat2MLw+z/nSWfUeI+24fnDZ68ubNT6q0ahKIrSSOS2yiC3lc2Ce3i/DmwvrWRHRSUDOrbihmP688aMNTzyxVJOe+RrKp2UHY9OtHNu9GiXxcBOrVi0sTgiXBasUMnOSGG/rq0bpN6qUSiKojQB7bLT6dm+BYM65xAKCT3ateDqI/pwygFdWLyxmMUbi0l1Jijqm5vNwM6t+PTGI8lIDZG3NTJF3nlPTOGUf3/dYHVVjUJRFKWZ0KZFOo9ddBAlZZVUVhnSU0N8+MM6Du3bnoxUm2SwVWYqT321gi8Xb+aNXxxKWkp4trtXvlvNRSPrfz6eJpkzu6HRObMVRdlTufSZ7/jK8WOkp4aiRnwvuPfEOk1s1BznzFYURVHqwN/OPZCdFVV8u6yARRuKef7blQDceGx/LhnVq0Fmv1NBoSiKshvRMScToGYA3gUjevDe9+u44ej+hEISb9c6o4JCURRlN2ZQ5xxuOyknccFdQKOeFEVRlLiooFAURVHiooJCURRFiYsKCkVRFCUuKigURVGUuKigUBRFUeKigkJRFEWJiwoKRVEUJS57ZK4nEckH6jqTeQdgcz1WZ3dAr3nvQK9576Cu19zLGJMbtGGPFBS7gojMiJUYa09Fr3nvQK9576AhrllNT4qiKEpcVFAoiqIocVFBEc2TTV2BJkCvee9Ar3nvoN6vWX0UiqIoSlxUo1AURVHiooJCURRFiYsKCgcROVFEFonIUhG5ranrU1+IyLMisklE5nrWtROR8SKyxPlu66wXEfm3cw9+EJGDmq7mdUdEeojIRBGZLyLzROTXzvo99rpFJFNEponIHOea/+is7yMi3znX9rqIpDvrM5zlpc723k16AbuAiKSIyPci8pGzvEdfs4isFJEfRWS2iMxw1jXos62CAvugAY8BJwGDgQtFZHDT1qreeB440bfuNmCCMaY/MMFZBnv9/Z3PtcB/G6mO9U0l8FtjzGBgFHCd83/uydddBhxtjDkQGAqcKCKjgD8D/zDG9AO2Alc75a8Gtjrr/+GU2135NbDAs7w3XPNRxpihnvESDftsG2P2+g9wKDDOs3w7cHtT16ser683MNezvAjo4vzuAixyfj8BXBhUbnf+AO8Dx+0t1w20AGYBI7EjdFOd9TXPOTAOONT5neqUk6auex2utbvTMB4NfATIXnDNK4EOvnUN+myrRmHpBqzxLOc56/ZUOhlj1ju/NwCdnN973H1wzAvDgO/Yw6/bMcHMBjYB44FlQKExptIp4r2ummt2thcB7Ru1wvXDP4FbgWpnuT17/jUb4DMRmSki1zrrGvTZTq1rTZU9A2OMEZE9MkZaRFoCbwM3GmO2iUjNtj3xuo0xVcBQEWkDvAsMatoaNSwiciqwyRgzU0TGNHF1GpMjjDFrRaQjMF5EFno3NsSzrRqFZS3Qw7Pc3Vm3p7JRRLoAON+bnPV7zH0QkTSskHjZGPOOs3qPv24AY0whMBFrdmkjIm6H0HtdNdfsbG8NFDRuTXeZw4HTRWQl8BrW/PQv9uxrxhiz1vnehO0QjKCBn20VFJbpQH8nWiIduAD4oInr1JB8AFzu/L4ca8N311/mREqMAoo86uxug1jV4RlggTHm755Ne+x1i0iuo0kgIllYn8wCrMA4xynmv2b3XpwDfGEcI/bugjHmdmNMd2NMb+w7+4Ux5mL24GsWkWwRaeX+Bo4H5tLQz3ZTO2aaywc4GViMteve2dT1qcfrehVYD1Rg7ZNXY+2yE4AlwOdAO6esYKO/lgE/AsObuv51vOYjsHbcH4DZzufkPfm6gQOA751rngvc7azvC0wDlgJvAhnO+kxneamzvW9TX8MuXv8Y4KM9/Zqda5vjfOa5bVVDP9uawkNRFEWJi5qeFEVRlLiooFAURVHiooJCURRFiYsKCkVRFCUuKigURVGUuKigUJRmhIiMcbOgKkpzQQWFoiiKEhcVFIpSB0TkEmf+h9ki8oSTkG+7iPzDmQ9igojkOmWHishUZz6Adz1zBfQTkc+dOSRmicg+zuFbishbIrJQRF4Wb5IqRWkCVFAoSi0RkX2B84HDjTFDgSrgYiAbmGGM2Q+YDPzB2eVF4HfGmAOwo2Pd9S8Djxk7h8Rh2BH0YLPd3oidG6UvNqeRojQZmj1WUWrPMcDBwHSns5+FTcJWDbzulPkf8I6ItAbaGGMmO+tfAN508vV0M8a8C2CMKQVwjjfNGJPnLM/GzifydYNflaLEQAWFotQeAV4wxtwesVLkLl+5uubHKfP8rkLfU6WJUdOTotSeCcA5znwA7nzFvbDvk5u19CLga2NMEbBVRI501l8KTDbGFAN5InKmc4wMEWnRmBehKMmiPRVFqSXGmPki8nvsLGMhbGbe64ASYISzbRPWjwE27fPjjiBYDlzprL8UeEJE7nWOcW4jXoaiJI1mj1WUekJEthtjWjZ1PRSlvlHTk6IoihIX1SgURVGUuKhGoSiKosRFBYWiKIoSFxUUiqIoSlxUUCiKoihxUUGhKIqixOX/AWhvqbkVFwX1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot di epoch vs loss su train(blu) e val(arancio)\n",
    "\n",
    "plt.plot(np.log(train_losses))\n",
    "plt.plot(np.log(val_losses))\n",
    "plt.legend([\"train\", \"val\"])\n",
    "plt.title(\"RNAseq Classification Model Training and Validation\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"log loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch with lowest val loss: 368\n",
      "train loss of said epoch: 0.1516905190035435\n",
      "val loss of said epoch: 0.35582235413091406\n"
     ]
    }
   ],
   "source": [
    "val_series = pd.Series(val_losses, dtype=float)\n",
    "\n",
    "print(f\"epoch with lowest val loss: {val_series.argmin()}\")\n",
    "print(f\"train loss of said epoch: {train_losses[val_series.argmin()]}\")\n",
    "print(f\"val loss of said epoch: {val_losses[val_series.argmin()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNAModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=5004, out_features=50, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoint = torch.load(\"RNA_class_epochs/368.pth\")\n",
    "\n",
    "best_model = RNAModel(input_dim=RNA_filtered.shape[0])\n",
    "best_model.load_state_dict(best_checkpoint[\"model\"])\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "\n",
    "ig = IntegratedGradients(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = []\n",
    "predicted = []\n",
    "attributes = []\n",
    "\n",
    "dataset.split= \"val\"\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 1, shuffle=True)\n",
    "\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "    \n",
    "    x_val = inputs.reshape([inputs.shape[0], 1, inputs.shape[1]])\n",
    "    \n",
    "    attributes.append(ig.attribute(inputs, target=0).numpy())\n",
    "    output = my_model(inputs)\n",
    "\n",
    "    expected.append(targets[0][0].item())\n",
    "    new_output = output.argmin()\n",
    "    predicted.append(new_output.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 1, 5004)\n"
     ]
    }
   ],
   "source": [
    "np_attributes = np.array(attributes)\n",
    "print(np_attributes.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0250927   0.00374682  0.22517412 ...  0.0524708  -0.00672203\n",
      "   0.08960086]]\n",
      "Index(['A2M', 'AAGAB', 'AAK1', 'AAMP', 'AARS', 'AARS2', 'AASDHPPT', 'AASS',\n",
      "       'AATF', 'ABCA1',\n",
      "       ...\n",
      "       'ZNHIT1', 'ZNHIT6', 'ZNRF2', 'ZRANB1', 'ZSWIM6', 'ZXDC', 'ZYG11B',\n",
      "       'ZYX', 'ZZEF1', 'ZZZ3'],\n",
      "      dtype='object', length=5004)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attr_mean = np_attributes.mean(axis=0)\n",
    "print(attr_mean)\n",
    "\n",
    "print(RNA_val.index)\n",
    "\n",
    "attr_series = pd.Series(attr_mean.ravel(),index = RNA_val.index).sort_values()\n",
    "\n",
    "best_attrs = attr_series[:10]\n",
    "worst_attrs = attr_series[-10:]\n",
    "\n",
    "attrs = pd.concat([best_attrs, worst_attrs])\n",
    "\n",
    "plt.bar(attrs.index, attrs)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 1]\n",
      " [1 4]]\n",
      "0.6888888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "mcc = matthews_corrcoef(expected, predicted)\n",
    "\n",
    "conf_matrix = confusion_matrix(expected,predicted)\n",
    "print(conf_matrix)\n",
    "\n",
    "print(mcc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
